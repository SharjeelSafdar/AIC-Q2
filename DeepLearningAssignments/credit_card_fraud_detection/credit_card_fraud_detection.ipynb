{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit-card-fraud-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IyGiOfo8mPeU",
        "lEnB_LPglxeW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScdsY98p-oVf"
      },
      "source": [
        "<h1 align=\"center\">DL Assignment: Credit Card Fraud Detection (Binary Classification)</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jQ1BPfJ31JG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk7FJv2HFa5n"
      },
      "source": [
        "# Loading Data and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "z6HZsRJTYBy1",
        "outputId": "1e396b24-11ea-48eb-85fd-4559c63e8407"
      },
      "source": [
        "credit_cards_df = pd.read_csv(\"./creditcard.csv\")\n",
        "credit_cards_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj65q3-4l7rx"
      },
      "source": [
        "## Checking for Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLQa6T4ZvoK"
      },
      "source": [
        "There are no missing values in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcoNTlC1Y8G1",
        "outputId": "b9134d54-4c79-447f-bcd9-0fdecfb9e6c8"
      },
      "source": [
        "credit_cards_df.isna().any().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KynxDy69mCDC"
      },
      "source": [
        "## Removing Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJQoQoHQkAkD"
      },
      "source": [
        "There are 1081 duplicate rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZwdE11wbxUF",
        "outputId": "5abf5e67-7dc9-4e68-93ae-4e76d0bc282b"
      },
      "source": [
        "credit_cards_df[credit_cards_df.duplicated()].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1081, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksKP98FClqHG"
      },
      "source": [
        "Dropping the duplicates..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT7AezgGkVJW"
      },
      "source": [
        "credit_cards_df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLeF3-FBmGjW"
      },
      "source": [
        "## Data Normalization and Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSLpQ3ZxZ-su"
      },
      "source": [
        "Minimum and Maximum values for each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "x6WGnJcNZ73-",
        "outputId": "afcdb338-adff-4d94-af90-56edd04374d7"
      },
      "source": [
        "data_stats = credit_cards_df.describe()\n",
        "data_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94811.077600</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.004135</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>-0.002966</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>-0.001139</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>-0.000854</td>\n",
              "      <td>-0.001596</td>\n",
              "      <td>-0.001441</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.000715</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>-0.000264</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.000371</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>-0.000232</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001763</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>88.472687</td>\n",
              "      <td>0.001667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47481.047891</td>\n",
              "      <td>1.948026</td>\n",
              "      <td>1.646703</td>\n",
              "      <td>1.508682</td>\n",
              "      <td>1.414184</td>\n",
              "      <td>1.377008</td>\n",
              "      <td>1.331931</td>\n",
              "      <td>1.227664</td>\n",
              "      <td>1.179054</td>\n",
              "      <td>1.095492</td>\n",
              "      <td>1.076407</td>\n",
              "      <td>1.018720</td>\n",
              "      <td>0.994674</td>\n",
              "      <td>0.995430</td>\n",
              "      <td>0.952215</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.873696</td>\n",
              "      <td>0.842507</td>\n",
              "      <td>0.837378</td>\n",
              "      <td>0.813379</td>\n",
              "      <td>0.769984</td>\n",
              "      <td>0.723909</td>\n",
              "      <td>0.724550</td>\n",
              "      <td>0.623702</td>\n",
              "      <td>0.605627</td>\n",
              "      <td>0.521220</td>\n",
              "      <td>0.482053</td>\n",
              "      <td>0.395744</td>\n",
              "      <td>0.328027</td>\n",
              "      <td>250.399437</td>\n",
              "      <td>0.040796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-56.407510</td>\n",
              "      <td>-72.715728</td>\n",
              "      <td>-48.325589</td>\n",
              "      <td>-5.683171</td>\n",
              "      <td>-113.743307</td>\n",
              "      <td>-26.160506</td>\n",
              "      <td>-43.557242</td>\n",
              "      <td>-73.216718</td>\n",
              "      <td>-13.434066</td>\n",
              "      <td>-24.588262</td>\n",
              "      <td>-4.797473</td>\n",
              "      <td>-18.683715</td>\n",
              "      <td>-5.791881</td>\n",
              "      <td>-19.214325</td>\n",
              "      <td>-4.498945</td>\n",
              "      <td>-14.129855</td>\n",
              "      <td>-25.162799</td>\n",
              "      <td>-9.498746</td>\n",
              "      <td>-7.213527</td>\n",
              "      <td>-54.497720</td>\n",
              "      <td>-34.830382</td>\n",
              "      <td>-10.933144</td>\n",
              "      <td>-44.807735</td>\n",
              "      <td>-2.836627</td>\n",
              "      <td>-10.295397</td>\n",
              "      <td>-2.604551</td>\n",
              "      <td>-22.565679</td>\n",
              "      <td>-15.430084</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54204.750000</td>\n",
              "      <td>-0.915951</td>\n",
              "      <td>-0.600321</td>\n",
              "      <td>-0.889682</td>\n",
              "      <td>-0.850134</td>\n",
              "      <td>-0.689830</td>\n",
              "      <td>-0.769031</td>\n",
              "      <td>-0.552509</td>\n",
              "      <td>-0.208828</td>\n",
              "      <td>-0.644221</td>\n",
              "      <td>-0.535578</td>\n",
              "      <td>-0.761649</td>\n",
              "      <td>-0.406198</td>\n",
              "      <td>-0.647862</td>\n",
              "      <td>-0.425732</td>\n",
              "      <td>-0.581452</td>\n",
              "      <td>-0.466860</td>\n",
              "      <td>-0.483928</td>\n",
              "      <td>-0.498014</td>\n",
              "      <td>-0.456289</td>\n",
              "      <td>-0.211469</td>\n",
              "      <td>-0.228305</td>\n",
              "      <td>-0.542700</td>\n",
              "      <td>-0.161703</td>\n",
              "      <td>-0.354453</td>\n",
              "      <td>-0.317485</td>\n",
              "      <td>-0.326763</td>\n",
              "      <td>-0.070641</td>\n",
              "      <td>-0.052818</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.500000</td>\n",
              "      <td>0.020384</td>\n",
              "      <td>0.063949</td>\n",
              "      <td>0.179963</td>\n",
              "      <td>-0.022248</td>\n",
              "      <td>-0.053468</td>\n",
              "      <td>-0.275168</td>\n",
              "      <td>0.040859</td>\n",
              "      <td>0.021898</td>\n",
              "      <td>-0.052596</td>\n",
              "      <td>-0.093237</td>\n",
              "      <td>-0.032306</td>\n",
              "      <td>0.139072</td>\n",
              "      <td>-0.012927</td>\n",
              "      <td>0.050209</td>\n",
              "      <td>0.049299</td>\n",
              "      <td>0.067119</td>\n",
              "      <td>-0.065867</td>\n",
              "      <td>-0.002142</td>\n",
              "      <td>0.003367</td>\n",
              "      <td>-0.062353</td>\n",
              "      <td>-0.029441</td>\n",
              "      <td>0.006675</td>\n",
              "      <td>-0.011159</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.016278</td>\n",
              "      <td>-0.052172</td>\n",
              "      <td>0.001479</td>\n",
              "      <td>0.011288</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139298.000000</td>\n",
              "      <td>1.316068</td>\n",
              "      <td>0.800283</td>\n",
              "      <td>1.026960</td>\n",
              "      <td>0.739647</td>\n",
              "      <td>0.612218</td>\n",
              "      <td>0.396792</td>\n",
              "      <td>0.570474</td>\n",
              "      <td>0.325704</td>\n",
              "      <td>0.595977</td>\n",
              "      <td>0.453619</td>\n",
              "      <td>0.739579</td>\n",
              "      <td>0.616976</td>\n",
              "      <td>0.663178</td>\n",
              "      <td>0.492336</td>\n",
              "      <td>0.650104</td>\n",
              "      <td>0.523512</td>\n",
              "      <td>0.398972</td>\n",
              "      <td>0.501956</td>\n",
              "      <td>0.458508</td>\n",
              "      <td>0.133207</td>\n",
              "      <td>0.186194</td>\n",
              "      <td>0.528245</td>\n",
              "      <td>0.147748</td>\n",
              "      <td>0.439738</td>\n",
              "      <td>0.350667</td>\n",
              "      <td>0.240261</td>\n",
              "      <td>0.091208</td>\n",
              "      <td>0.078276</td>\n",
              "      <td>77.510000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930</td>\n",
              "      <td>22.057729</td>\n",
              "      <td>9.382558</td>\n",
              "      <td>16.875344</td>\n",
              "      <td>34.801666</td>\n",
              "      <td>73.301626</td>\n",
              "      <td>120.589494</td>\n",
              "      <td>20.007208</td>\n",
              "      <td>15.594995</td>\n",
              "      <td>23.745136</td>\n",
              "      <td>12.018913</td>\n",
              "      <td>7.848392</td>\n",
              "      <td>7.126883</td>\n",
              "      <td>10.526766</td>\n",
              "      <td>8.877742</td>\n",
              "      <td>17.315112</td>\n",
              "      <td>9.253526</td>\n",
              "      <td>5.041069</td>\n",
              "      <td>5.591971</td>\n",
              "      <td>39.420904</td>\n",
              "      <td>27.202839</td>\n",
              "      <td>10.503090</td>\n",
              "      <td>22.528412</td>\n",
              "      <td>4.584549</td>\n",
              "      <td>7.519589</td>\n",
              "      <td>3.517346</td>\n",
              "      <td>31.612198</td>\n",
              "      <td>33.847808</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time             V1  ...         Amount          Class\n",
              "count  283726.000000  283726.000000  ...  283726.000000  283726.000000\n",
              "mean    94811.077600       0.005917  ...      88.472687       0.001667\n",
              "std     47481.047891       1.948026  ...     250.399437       0.040796\n",
              "min         0.000000     -56.407510  ...       0.000000       0.000000\n",
              "25%     54204.750000      -0.915951  ...       5.600000       0.000000\n",
              "50%     84692.500000       0.020384  ...      22.000000       0.000000\n",
              "75%    139298.000000       1.316068  ...      77.510000       0.000000\n",
              "max    172792.000000       2.454930  ...   25691.160000       1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2nwMY2ruo2J"
      },
      "source": [
        "Values of Columns V1 through V28 have different ranges: we need to normalize them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVwPKMtRlj-1"
      },
      "source": [
        "credit_cards_df.loc[:, \"V1\": \"V28\"] = credit_cards_df.loc[:, \"V1\": \"V28\"].sub(\n",
        "    credit_cards_df.describe().loc[\"mean\", \"V1\": \"V28\"]\n",
        ")\n",
        "credit_cards_df.loc[:, \"V1\": \"V28\"] = credit_cards_df.loc[:, \"V1\": \"V28\"].div(\n",
        "    credit_cards_df.describe().loc[\"std\", \"V1\": \"V28\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6zsLUwQjN2m"
      },
      "source": [
        "Map \"Time\" values to the range [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCZviBEhVvi1"
      },
      "source": [
        "credit_cards_df.loc[:, \"Time\"] = credit_cards_df.loc[:, \"Time\"].astype(\"float64\") / np.max(credit_cards_df.loc[:, \"Time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVTmkaGcjcN0"
      },
      "source": [
        "Convert \"Class\" column to float64 to make the data homogeneous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "AgyXfzW-jp8n",
        "outputId": "32b8c9ce-8f47-4a63-a740-c3fb22614b68"
      },
      "source": [
        "credit_cards_df.loc[:, \"Class\"] = credit_cards_df.loc[:, \"Class\"].astype(np.float64)\n",
        "credit_cards_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.701081</td>\n",
              "      <td>-0.041687</td>\n",
              "      <td>1.680098</td>\n",
              "      <td>0.976621</td>\n",
              "      <td>-0.247020</td>\n",
              "      <td>0.348012</td>\n",
              "      <td>0.193699</td>\n",
              "      <td>0.084434</td>\n",
              "      <td>0.333533</td>\n",
              "      <td>0.085688</td>\n",
              "      <td>-0.541661</td>\n",
              "      <td>-0.620390</td>\n",
              "      <td>-0.996548</td>\n",
              "      <td>-0.327050</td>\n",
              "      <td>1.603612</td>\n",
              "      <td>-0.539733</td>\n",
              "      <td>0.246646</td>\n",
              "      <td>0.028990</td>\n",
              "      <td>0.497010</td>\n",
              "      <td>0.326273</td>\n",
              "      <td>-0.024777</td>\n",
              "      <td>0.383483</td>\n",
              "      <td>-0.177444</td>\n",
              "      <td>0.110157</td>\n",
              "      <td>0.247058</td>\n",
              "      <td>-0.392621</td>\n",
              "      <td>0.333032</td>\n",
              "      <td>-0.065849</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.608791</td>\n",
              "      <td>0.164137</td>\n",
              "      <td>0.109279</td>\n",
              "      <td>0.318997</td>\n",
              "      <td>0.042258</td>\n",
              "      <td>-0.060980</td>\n",
              "      <td>-0.065656</td>\n",
              "      <td>0.072903</td>\n",
              "      <td>-0.231703</td>\n",
              "      <td>-0.153784</td>\n",
              "      <td>1.582893</td>\n",
              "      <td>1.071657</td>\n",
              "      <td>0.490734</td>\n",
              "      <td>-0.151252</td>\n",
              "      <td>0.693540</td>\n",
              "      <td>0.529652</td>\n",
              "      <td>-0.136467</td>\n",
              "      <td>-0.220780</td>\n",
              "      <td>-0.178907</td>\n",
              "      <td>-0.089963</td>\n",
              "      <td>-0.311371</td>\n",
              "      <td>-0.881453</td>\n",
              "      <td>0.162080</td>\n",
              "      <td>-0.561502</td>\n",
              "      <td>0.321175</td>\n",
              "      <td>0.260853</td>\n",
              "      <td>-0.027154</td>\n",
              "      <td>0.043219</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.700335</td>\n",
              "      <td>-0.811335</td>\n",
              "      <td>1.174268</td>\n",
              "      <td>0.270648</td>\n",
              "      <td>-0.366756</td>\n",
              "      <td>1.352652</td>\n",
              "      <td>0.643222</td>\n",
              "      <td>0.210788</td>\n",
              "      <td>-1.381167</td>\n",
              "      <td>0.194242</td>\n",
              "      <td>0.612827</td>\n",
              "      <td>0.067156</td>\n",
              "      <td>0.719980</td>\n",
              "      <td>-0.174539</td>\n",
              "      <td>2.562945</td>\n",
              "      <td>-3.309211</td>\n",
              "      <td>1.317258</td>\n",
              "      <td>-0.146737</td>\n",
              "      <td>-2.780492</td>\n",
              "      <td>0.681563</td>\n",
              "      <td>0.343094</td>\n",
              "      <td>1.065067</td>\n",
              "      <td>1.457769</td>\n",
              "      <td>-1.138482</td>\n",
              "      <td>-0.628159</td>\n",
              "      <td>-0.288860</td>\n",
              "      <td>-0.144325</td>\n",
              "      <td>-0.183824</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.499064</td>\n",
              "      <td>-0.109972</td>\n",
              "      <td>1.187381</td>\n",
              "      <td>-0.608354</td>\n",
              "      <td>-0.008814</td>\n",
              "      <td>0.937243</td>\n",
              "      <td>0.192079</td>\n",
              "      <td>0.320842</td>\n",
              "      <td>-1.264662</td>\n",
              "      <td>-0.049713</td>\n",
              "      <td>-0.222523</td>\n",
              "      <td>0.179901</td>\n",
              "      <td>0.509482</td>\n",
              "      <td>-0.302638</td>\n",
              "      <td>-0.691295</td>\n",
              "      <td>-1.214162</td>\n",
              "      <td>-0.812174</td>\n",
              "      <td>2.345728</td>\n",
              "      <td>-1.515110</td>\n",
              "      <td>-0.270428</td>\n",
              "      <td>-0.149093</td>\n",
              "      <td>0.007299</td>\n",
              "      <td>-0.305464</td>\n",
              "      <td>-1.941443</td>\n",
              "      <td>1.242485</td>\n",
              "      <td>-0.460693</td>\n",
              "      <td>0.154039</td>\n",
              "      <td>0.185687</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.597605</td>\n",
              "      <td>0.535538</td>\n",
              "      <td>1.025468</td>\n",
              "      <td>0.287092</td>\n",
              "      <td>-0.297036</td>\n",
              "      <td>0.072872</td>\n",
              "      <td>0.481516</td>\n",
              "      <td>-0.228724</td>\n",
              "      <td>0.747915</td>\n",
              "      <td>0.700957</td>\n",
              "      <td>-0.807920</td>\n",
              "      <td>0.541796</td>\n",
              "      <td>1.351425</td>\n",
              "      <td>-1.176123</td>\n",
              "      <td>0.190272</td>\n",
              "      <td>-0.518042</td>\n",
              "      <td>-0.281545</td>\n",
              "      <td>-0.047422</td>\n",
              "      <td>0.988164</td>\n",
              "      <td>0.530342</td>\n",
              "      <td>-0.012516</td>\n",
              "      <td>1.101778</td>\n",
              "      <td>-0.220708</td>\n",
              "      <td>0.232904</td>\n",
              "      <td>-0.394799</td>\n",
              "      <td>1.041676</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.654233</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0  0.000000 -0.701081 -0.041687  1.680098  ...  0.333032 -0.065849  149.62    0.0\n",
              "1  0.000000  0.608791  0.164137  0.109279  ... -0.027154  0.043219    2.69    0.0\n",
              "2  0.000006 -0.700335 -0.811335  1.174268  ... -0.144325 -0.183824  378.66    0.0\n",
              "3  0.000006 -0.499064 -0.109972  1.187381  ...  0.154039  0.185687  123.50    0.0\n",
              "4  0.000012 -0.597605  0.535538  1.025468  ...  0.550000  0.654233   69.99    0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "MX6TV-JXzuyS",
        "outputId": "05d8bad3-3ced-436c-fe45-f1f3b9aadec7"
      },
      "source": [
        "credit_cards_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>283726.000000</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>2.837260e+05</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.548701</td>\n",
              "      <td>-2.163896e-15</td>\n",
              "      <td>1.723077e-16</td>\n",
              "      <td>3.161866e-15</td>\n",
              "      <td>-9.771857e-16</td>\n",
              "      <td>3.252123e-15</td>\n",
              "      <td>-2.658013e-16</td>\n",
              "      <td>1.479581e-15</td>\n",
              "      <td>3.017235e-16</td>\n",
              "      <td>1.449294e-15</td>\n",
              "      <td>4.549020e-16</td>\n",
              "      <td>1.460529e-15</td>\n",
              "      <td>3.604078e-16</td>\n",
              "      <td>-1.036357e-15</td>\n",
              "      <td>-8.759689e-16</td>\n",
              "      <td>4.328131e-15</td>\n",
              "      <td>2.161332e-17</td>\n",
              "      <td>3.248840e-16</td>\n",
              "      <td>7.056002e-16</td>\n",
              "      <td>1.547330e-16</td>\n",
              "      <td>3.924582e-16</td>\n",
              "      <td>3.193375e-16</td>\n",
              "      <td>-1.630733e-15</td>\n",
              "      <td>-6.421451e-16</td>\n",
              "      <td>-2.211837e-17</td>\n",
              "      <td>-2.232311e-15</td>\n",
              "      <td>-1.248478e-16</td>\n",
              "      <td>-1.893228e-16</td>\n",
              "      <td>-2.805617e-17</td>\n",
              "      <td>88.472687</td>\n",
              "      <td>0.001667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.274787</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>250.399437</td>\n",
              "      <td>0.040796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.895928e+01</td>\n",
              "      <td>-4.415586e+01</td>\n",
              "      <td>-3.203273e+01</td>\n",
              "      <td>-4.016595e+00</td>\n",
              "      <td>-8.260309e+01</td>\n",
              "      <td>-1.964019e+01</td>\n",
              "      <td>-3.548124e+01</td>\n",
              "      <td>-6.209710e+01</td>\n",
              "      <td>-1.226158e+01</td>\n",
              "      <td>-2.284156e+01</td>\n",
              "      <td>-4.709512e+00</td>\n",
              "      <td>-1.878303e+01</td>\n",
              "      <td>-5.819080e+00</td>\n",
              "      <td>-2.017882e+01</td>\n",
              "      <td>-4.918591e+00</td>\n",
              "      <td>-1.617383e+01</td>\n",
              "      <td>-2.986677e+01</td>\n",
              "      <td>-1.134525e+01</td>\n",
              "      <td>-8.868273e+00</td>\n",
              "      <td>-7.077795e+01</td>\n",
              "      <td>-4.811377e+01</td>\n",
              "      <td>-1.508953e+01</td>\n",
              "      <td>-7.184185e+01</td>\n",
              "      <td>-4.684141e+00</td>\n",
              "      <td>-1.975204e+01</td>\n",
              "      <td>-5.403348e+00</td>\n",
              "      <td>-5.702537e+01</td>\n",
              "      <td>-4.704079e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.313699</td>\n",
              "      <td>-4.732321e-01</td>\n",
              "      <td>-3.620482e-01</td>\n",
              "      <td>-5.907774e-01</td>\n",
              "      <td>-5.990509e-01</td>\n",
              "      <td>-5.022899e-01</td>\n",
              "      <td>-5.765250e-01</td>\n",
              "      <td>-4.515162e-01</td>\n",
              "      <td>-1.763905e-01</td>\n",
              "      <td>-5.866079e-01</td>\n",
              "      <td>-4.962224e-01</td>\n",
              "      <td>-7.478512e-01</td>\n",
              "      <td>-4.076539e-01</td>\n",
              "      <td>-6.514428e-01</td>\n",
              "      <td>-4.473619e-01</td>\n",
              "      <td>-6.366802e-01</td>\n",
              "      <td>-5.356800e-01</td>\n",
              "      <td>-5.745917e-01</td>\n",
              "      <td>-5.965403e-01</td>\n",
              "      <td>-5.606549e-01</td>\n",
              "      <td>-2.748841e-01</td>\n",
              "      <td>-3.148658e-01</td>\n",
              "      <td>-7.489950e-01</td>\n",
              "      <td>-2.595815e-01</td>\n",
              "      <td>-5.856209e-01</td>\n",
              "      <td>-6.086735e-01</td>\n",
              "      <td>-6.781679e-01</td>\n",
              "      <td>-1.829563e-01</td>\n",
              "      <td>-1.626860e-01</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.490141</td>\n",
              "      <td>7.426443e-03</td>\n",
              "      <td>4.134560e-02</td>\n",
              "      <td>1.182155e-01</td>\n",
              "      <td>-1.363451e-02</td>\n",
              "      <td>-4.015602e-02</td>\n",
              "      <td>-2.057376e-01</td>\n",
              "      <td>3.181553e-02</td>\n",
              "      <td>1.929716e-02</td>\n",
              "      <td>-4.655390e-02</td>\n",
              "      <td>-8.527981e-02</td>\n",
              "      <td>-3.191033e-02</td>\n",
              "      <td>1.405352e-01</td>\n",
              "      <td>-1.359249e-02</td>\n",
              "      <td>5.246332e-02</td>\n",
              "      <td>5.274494e-02</td>\n",
              "      <td>7.549205e-02</td>\n",
              "      <td>-7.838163e-02</td>\n",
              "      <td>-4.367113e-03</td>\n",
              "      <td>4.464522e-03</td>\n",
              "      <td>-8.122327e-02</td>\n",
              "      <td>-4.015696e-02</td>\n",
              "      <td>9.233205e-03</td>\n",
              "      <td>-1.820860e-02</td>\n",
              "      <td>6.737075e-02</td>\n",
              "      <td>3.167710e-02</td>\n",
              "      <td>-1.085380e-01</td>\n",
              "      <td>-7.187396e-04</td>\n",
              "      <td>3.274214e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.806160</td>\n",
              "      <td>6.725529e-01</td>\n",
              "      <td>4.885020e-01</td>\n",
              "      <td>6.796309e-01</td>\n",
              "      <td>5.251179e-01</td>\n",
              "      <td>4.432729e-01</td>\n",
              "      <td>2.987630e-01</td>\n",
              "      <td>4.632157e-01</td>\n",
              "      <td>2.769661e-01</td>\n",
              "      <td>5.454835e-01</td>\n",
              "      <td>4.227576e-01</td>\n",
              "      <td>7.257905e-01</td>\n",
              "      <td>6.209984e-01</td>\n",
              "      <td>6.656170e-01</td>\n",
              "      <td>5.167774e-01</td>\n",
              "      <td>7.094391e-01</td>\n",
              "      <td>5.978617e-01</td>\n",
              "      <td>4.733508e-01</td>\n",
              "      <td>5.976283e-01</td>\n",
              "      <td>5.640328e-01</td>\n",
              "      <td>1.727567e-01</td>\n",
              "      <td>2.577176e-01</td>\n",
              "      <td>7.290869e-01</td>\n",
              "      <td>2.365716e-01</td>\n",
              "      <td>7.257346e-01</td>\n",
              "      <td>6.732273e-01</td>\n",
              "      <td>4.981027e-01</td>\n",
              "      <td>2.260177e-01</td>\n",
              "      <td>2.369588e-01</td>\n",
              "      <td>77.510000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.257177e+00</td>\n",
              "      <td>1.339760e+01</td>\n",
              "      <td>6.217974e+00</td>\n",
              "      <td>1.193502e+01</td>\n",
              "      <td>2.527206e+01</td>\n",
              "      <td>5.503497e+01</td>\n",
              "      <td>9.822533e+01</td>\n",
              "      <td>1.696958e+01</td>\n",
              "      <td>1.423706e+01</td>\n",
              "      <td>2.206096e+01</td>\n",
              "      <td>1.179785e+01</td>\n",
              "      <td>7.891132e+00</td>\n",
              "      <td>7.158999e+00</td>\n",
              "      <td>1.105476e+01</td>\n",
              "      <td>9.702438e+00</td>\n",
              "      <td>1.981690e+01</td>\n",
              "      <td>1.098312e+01</td>\n",
              "      <td>6.018258e+00</td>\n",
              "      <td>6.875317e+00</td>\n",
              "      <td>5.119678e+01</td>\n",
              "      <td>3.757820e+01</td>\n",
              "      <td>1.449603e+01</td>\n",
              "      <td>3.612013e+01</td>\n",
              "      <td>7.569572e+00</td>\n",
              "      <td>1.442734e+01</td>\n",
              "      <td>7.296286e+00</td>\n",
              "      <td>7.987599e+01</td>\n",
              "      <td>1.031845e+02</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time            V1  ...         Amount          Class\n",
              "count  283726.000000  2.837260e+05  ...  283726.000000  283726.000000\n",
              "mean        0.548701 -2.163896e-15  ...      88.472687       0.001667\n",
              "std         0.274787  1.000000e+00  ...     250.399437       0.040796\n",
              "min         0.000000 -2.895928e+01  ...       0.000000       0.000000\n",
              "25%         0.313699 -4.732321e-01  ...       5.600000       0.000000\n",
              "50%         0.490141  7.426443e-03  ...      22.000000       0.000000\n",
              "75%         0.806160  6.725529e-01  ...      77.510000       0.000000\n",
              "max         1.000000  1.257177e+00  ...   25691.160000       1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqdu5rU34NRu",
        "outputId": "8b0e3a32-6bd5-4fb9-c50a-b0042eeb1757"
      },
      "source": [
        "credit_cards_df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      float64\n",
              "V1        float64\n",
              "V2        float64\n",
              "V3        float64\n",
              "V4        float64\n",
              "V5        float64\n",
              "V6        float64\n",
              "V7        float64\n",
              "V8        float64\n",
              "V9        float64\n",
              "V10       float64\n",
              "V11       float64\n",
              "V12       float64\n",
              "V13       float64\n",
              "V14       float64\n",
              "V15       float64\n",
              "V16       float64\n",
              "V17       float64\n",
              "V18       float64\n",
              "V19       float64\n",
              "V20       float64\n",
              "V21       float64\n",
              "V22       float64\n",
              "V23       float64\n",
              "V24       float64\n",
              "V25       float64\n",
              "V26       float64\n",
              "V27       float64\n",
              "V28       float64\n",
              "Amount    float64\n",
              "Class     float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyGiOfo8mPeU"
      },
      "source": [
        "## Separating Data into Training and Test Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq9l8FwQ8cjh"
      },
      "source": [
        "The number of fraudulent transactions is very small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5IC76eNfq2C",
        "outputId": "a955bd86-de9f-4ed1-ce20-ecd14dfa28ef"
      },
      "source": [
        "credit_cards_df[\"Class\"].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_1OMnDWhGuL"
      },
      "source": [
        "Let's separate fraudulent and legit transactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtaKQiLNg3f9"
      },
      "source": [
        "data_fraudulent = credit_cards_df.loc[credit_cards_df[\"Class\"] == 1, :]\n",
        "data_not_fraudulent = credit_cards_df.loc[credit_cards_df[\"Class\"] == 0, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ecHcXcRhkB3"
      },
      "source": [
        "Combine 50% of the fraudulent and 50% of the legit transactions to make our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRdDcUWNiUsL"
      },
      "source": [
        "train_data = np.vstack((\n",
        "    data_fraudulent[: int(data_fraudulent.shape[0] * 0.5)],\n",
        "    data_not_fraudulent[: int(data_not_fraudulent.shape[0] * 0.5)],\n",
        "))\n",
        "np.random.shuffle(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY1yTsnMleHO"
      },
      "source": [
        "Combine 20% of the fraudulent and 20% of the legit transactions to make our validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpeEcGmxlCw_"
      },
      "source": [
        "val_data = np.vstack((\n",
        "    data_fraudulent[int(data_fraudulent.shape[0] * 0.5) : int(data_fraudulent.shape[0] * 0.7)],\n",
        "    data_not_fraudulent[int(data_not_fraudulent.shape[0] * 0.5) : int(data_not_fraudulent.shape[0] * 0.7)]\n",
        "))\n",
        "np.random.shuffle(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnofYaJdjprj"
      },
      "source": [
        "Combine 30% of the fraudulent and 30% of the legit transactions to make our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C95qB_thjn4s"
      },
      "source": [
        "test_data = np.vstack((\n",
        "    data_fraudulent[int(data_fraudulent.shape[0] * 0.7) :],\n",
        "    data_not_fraudulent[int(data_not_fraudulent.shape[0] * 0.7) :]\n",
        "))\n",
        "np.random.shuffle(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEnB_LPglxeW"
      },
      "source": [
        "## Separating Training and Testing Datasets into Samples and Labels Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T7goZ0DO76J"
      },
      "source": [
        "Training dataset excluding the data for validation (50% of total samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YinpW8M-eoNf",
        "outputId": "1964b43d-a797-4d99-b3f8-f9bb7007f1e1"
      },
      "source": [
        "x_train_partial = train_data[:, :-1]\n",
        "print(\"x_train_partial.shape = \", x_train_partial.shape)\n",
        "print(\"x_train_partial.dtype = \", x_train_partial.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_partial.shape =  (141862, 30)\n",
            "x_train_partial.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_dVtLYRgm33",
        "outputId": "49a7f238-fa73-4203-87cb-005dcc9426bb"
      },
      "source": [
        "y_train_partial = train_data[:, -1]\n",
        "print(\"y_train_partial.shape = \", y_train_partial.shape)\n",
        "print(\"y_train_partial.dtype = \", y_train_partial.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train_partial.shape =  (141862,)\n",
            "y_train_partial.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7cApLzmPEzQ"
      },
      "source": [
        "Validation dataset (20% of total samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM4Gtt_ILj1o",
        "outputId": "dc07140f-3127-4c81-8926-e78e690abc7e"
      },
      "source": [
        "x_val = val_data[:, :-1]\n",
        "print(\"x_val.shape = \", x_val.shape)\n",
        "print(\"x_val.dtype = \", x_val.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_val.shape =  (56746, 30)\n",
            "x_val.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-o7MVV-Ljd5",
        "outputId": "ed6dcf08-d628-4c5e-a274-389844d30308"
      },
      "source": [
        "y_val = val_data[:, -1]\n",
        "print(\"y_val.shape = \", y_val.shape)\n",
        "print(\"y_val.dtype = \", y_val.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_val.shape =  (56746,)\n",
            "y_val.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79CB4_-MPTQo"
      },
      "source": [
        "Complete training dataset, including the validation dataset (70% of total samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khvpiujEMFlR",
        "outputId": "698498d9-2f87-4099-9c94-7b61d272064f"
      },
      "source": [
        "x_train = np.vstack((x_train_partial, x_val))\n",
        "print(\"x_train.shape = \", x_train.shape)\n",
        "print(\"x_train.dtype = \", x_train.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape =  (198608, 30)\n",
            "x_train.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2CZr00tMFaZ",
        "outputId": "3b8f12c5-8edd-4d98-cd86-09720a7455e5"
      },
      "source": [
        "y_train = np.hstack((y_train_partial, y_val))\n",
        "print(\"y_train.shape = \", y_train.shape)\n",
        "print(\"y_train.dtype = \", y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train.shape =  (198608,)\n",
            "y_train.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ZgTAWfPlpg"
      },
      "source": [
        "Test dataset (30% of total samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yyMgFwwis_Y",
        "outputId": "43bebb52-d4f7-48bc-e5b9-b77c06aa97fd"
      },
      "source": [
        "x_test = test_data[:, :-1]\n",
        "print(\"x_test.shape = \", x_test.shape)\n",
        "print(\"x_test.dtype = \", x_test.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test.shape =  (85118, 30)\n",
            "x_test.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ2hbrs3ljly",
        "outputId": "12965f85-4468-4a32-823e-a4c9f109b167"
      },
      "source": [
        "y_test = test_data[:, -1]\n",
        "print(\"y_test.shape = \", y_test.shape)\n",
        "print(\"y_test.dtype = \", y_test.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_test.shape =  (85118,)\n",
            "y_test.dtype =  float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcZhKqiP5Wp"
      },
      "source": [
        "# Building the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VEazBZCxjHG"
      },
      "source": [
        "First let us build a model according to the instructions in the assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJzzDaz-QBGH"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
        "model.add(layers.Dense(8, activation='relu'))\n",
        "model.add(layers.Dense(6, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjaY2vBpQ1hF"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcgxCvDol6RH"
      },
      "source": [
        "# Training and Validating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPSYvAHpRHHM",
        "outputId": "73aadad4-07af-40ee-9e96-cd5a4568fccc"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train_partial,\n",
        "    y_train_partial,\n",
        "    batch_size=512,\n",
        "    epochs=100,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 1.2514 - accuracy: 0.4910 - val_loss: 0.0221 - val_accuracy: 0.9965\n",
            "Epoch 2/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0213 - accuracy: 0.9973 - val_loss: 0.0206 - val_accuracy: 0.9972\n",
            "Epoch 3/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9976 - val_loss: 0.0198 - val_accuracy: 0.9977\n",
            "Epoch 4/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9980 - val_loss: 0.0194 - val_accuracy: 0.9977\n",
            "Epoch 5/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.0193 - val_accuracy: 0.9975\n",
            "Epoch 6/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9955\n",
            "Epoch 7/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9978 - val_loss: 0.0190 - val_accuracy: 0.9978\n",
            "Epoch 8/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9979 - val_loss: 0.0202 - val_accuracy: 0.9866\n",
            "Epoch 9/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9974 - val_loss: 0.0188 - val_accuracy: 0.9972\n",
            "Epoch 10/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9979 - val_loss: 0.0187 - val_accuracy: 0.9977\n",
            "Epoch 11/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9981 - val_loss: 0.0188 - val_accuracy: 0.9977\n",
            "Epoch 12/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9979 - val_loss: 0.0189 - val_accuracy: 0.9940\n",
            "Epoch 13/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.0190 - val_accuracy: 0.9952\n",
            "Epoch 14/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9981 - val_loss: 0.0189 - val_accuracy: 0.9977\n",
            "Epoch 15/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9978 - val_loss: 0.0194 - val_accuracy: 0.9923\n",
            "Epoch 16/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9974 - val_loss: 0.0189 - val_accuracy: 0.9978\n",
            "Epoch 17/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9975 - val_loss: 0.0188 - val_accuracy: 0.9978\n",
            "Epoch 18/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9981 - val_loss: 0.0188 - val_accuracy: 0.9966\n",
            "Epoch 19/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9980 - val_loss: 0.0188 - val_accuracy: 0.9977\n",
            "Epoch 20/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9975 - val_loss: 0.0188 - val_accuracy: 0.9973\n",
            "Epoch 21/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9979 - val_loss: 0.0186 - val_accuracy: 0.9967\n",
            "Epoch 22/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9979 - val_loss: 0.0187 - val_accuracy: 0.9972\n",
            "Epoch 23/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9981 - val_loss: 0.0190 - val_accuracy: 0.9976\n",
            "Epoch 24/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9982 - val_loss: 0.0188 - val_accuracy: 0.9975\n",
            "Epoch 25/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9972 - val_loss: 0.0187 - val_accuracy: 0.9964\n",
            "Epoch 26/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9980 - val_loss: 0.0187 - val_accuracy: 0.9953\n",
            "Epoch 27/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9978 - val_loss: 0.0187 - val_accuracy: 0.9966\n",
            "Epoch 28/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9979 - val_loss: 0.0186 - val_accuracy: 0.9967\n",
            "Epoch 29/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9979 - val_loss: 0.0191 - val_accuracy: 0.9909\n",
            "Epoch 30/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0189 - val_accuracy: 0.9972\n",
            "Epoch 31/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9979 - val_loss: 0.0185 - val_accuracy: 0.9965\n",
            "Epoch 32/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9979 - val_loss: 0.0188 - val_accuracy: 0.9968\n",
            "Epoch 33/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.0186 - val_accuracy: 0.9955\n",
            "Epoch 34/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.0194 - val_accuracy: 0.9853\n",
            "Epoch 35/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9978 - val_loss: 0.0191 - val_accuracy: 0.9884\n",
            "Epoch 36/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9977 - val_loss: 0.0187 - val_accuracy: 0.9954\n",
            "Epoch 37/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.0189 - val_accuracy: 0.9949\n",
            "Epoch 38/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9979 - val_loss: 0.0187 - val_accuracy: 0.9950\n",
            "Epoch 39/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9979 - val_loss: 0.0189 - val_accuracy: 0.9952\n",
            "Epoch 40/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9980 - val_loss: 0.0194 - val_accuracy: 0.9881\n",
            "Epoch 41/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9979 - val_loss: 0.0198 - val_accuracy: 0.9830\n",
            "Epoch 42/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9965 - val_loss: 0.0195 - val_accuracy: 0.9881\n",
            "Epoch 43/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9980 - val_loss: 0.0187 - val_accuracy: 0.9955\n",
            "Epoch 44/100\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9978 - val_loss: 0.0189 - val_accuracy: 0.9940\n",
            "Epoch 45/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9981 - val_loss: 0.0191 - val_accuracy: 0.9921\n",
            "Epoch 46/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9978 - val_loss: 0.0191 - val_accuracy: 0.9916\n",
            "Epoch 47/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9980 - val_loss: 0.0205 - val_accuracy: 0.9854\n",
            "Epoch 48/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9978 - val_loss: 0.0200 - val_accuracy: 0.9860\n",
            "Epoch 49/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9973 - val_loss: 0.0189 - val_accuracy: 0.9933\n",
            "Epoch 50/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9978 - val_loss: 0.0196 - val_accuracy: 0.9873\n",
            "Epoch 51/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9979 - val_loss: 0.0191 - val_accuracy: 0.9913\n",
            "Epoch 52/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9921\n",
            "Epoch 53/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9979 - val_loss: 0.0192 - val_accuracy: 0.9905\n",
            "Epoch 54/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9977 - val_loss: 0.0198 - val_accuracy: 0.9864\n",
            "Epoch 55/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9927\n",
            "Epoch 56/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9930\n",
            "Epoch 57/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9979 - val_loss: 0.0206 - val_accuracy: 0.9822\n",
            "Epoch 58/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9980 - val_loss: 0.0194 - val_accuracy: 0.9904\n",
            "Epoch 59/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9979 - val_loss: 0.0194 - val_accuracy: 0.9896\n",
            "Epoch 60/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.0203 - val_accuracy: 0.9854\n",
            "Epoch 61/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9979 - val_loss: 0.0206 - val_accuracy: 0.9835\n",
            "Epoch 62/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9978 - val_loss: 0.0197 - val_accuracy: 0.9873\n",
            "Epoch 63/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0199 - val_accuracy: 0.9881\n",
            "Epoch 64/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9981 - val_loss: 0.0193 - val_accuracy: 0.9910\n",
            "Epoch 65/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9982 - val_loss: 0.0194 - val_accuracy: 0.9907\n",
            "Epoch 66/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9978 - val_loss: 0.0195 - val_accuracy: 0.9904\n",
            "Epoch 67/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9980 - val_loss: 0.0222 - val_accuracy: 0.9778\n",
            "Epoch 68/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9977 - val_loss: 0.0190 - val_accuracy: 0.9942\n",
            "Epoch 69/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9982 - val_loss: 0.0189 - val_accuracy: 0.9941\n",
            "Epoch 70/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.0195 - val_accuracy: 0.9901\n",
            "Epoch 71/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9979 - val_loss: 0.0197 - val_accuracy: 0.9893\n",
            "Epoch 72/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9979 - val_loss: 0.0193 - val_accuracy: 0.9905\n",
            "Epoch 73/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9979 - val_loss: 0.0193 - val_accuracy: 0.9907\n",
            "Epoch 74/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9980 - val_loss: 0.0195 - val_accuracy: 0.9915\n",
            "Epoch 75/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9980 - val_loss: 0.0195 - val_accuracy: 0.9918\n",
            "Epoch 76/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9981 - val_loss: 0.0200 - val_accuracy: 0.9891\n",
            "Epoch 77/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.0198 - val_accuracy: 0.9898\n",
            "Epoch 78/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9978 - val_loss: 0.0193 - val_accuracy: 0.9927\n",
            "Epoch 79/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9981 - val_loss: 0.0201 - val_accuracy: 0.9893\n",
            "Epoch 80/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9979 - val_loss: 0.0209 - val_accuracy: 0.9881\n",
            "Epoch 81/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9981 - val_loss: 0.0207 - val_accuracy: 0.9885\n",
            "Epoch 82/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9980 - val_loss: 0.0213 - val_accuracy: 0.9870\n",
            "Epoch 83/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.0206 - val_accuracy: 0.9887\n",
            "Epoch 84/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0213 - val_accuracy: 0.9873\n",
            "Epoch 85/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.0201 - val_accuracy: 0.9902\n",
            "Epoch 86/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9979 - val_loss: 0.0236 - val_accuracy: 0.9795\n",
            "Epoch 87/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9978 - val_loss: 0.0212 - val_accuracy: 0.9869\n",
            "Epoch 88/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9981 - val_loss: 0.0232 - val_accuracy: 0.9814\n",
            "Epoch 89/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9981 - val_loss: 0.0194 - val_accuracy: 0.9911\n",
            "Epoch 90/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9978 - val_loss: 0.0202 - val_accuracy: 0.9895\n",
            "Epoch 91/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 0.9842\n",
            "Epoch 92/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9981 - val_loss: 0.0208 - val_accuracy: 0.9867\n",
            "Epoch 93/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9982 - val_loss: 0.0208 - val_accuracy: 0.9859\n",
            "Epoch 94/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9979 - val_loss: 0.0195 - val_accuracy: 0.9913\n",
            "Epoch 95/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9981 - val_loss: 0.0195 - val_accuracy: 0.9917\n",
            "Epoch 96/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9980 - val_loss: 0.0218 - val_accuracy: 0.9845\n",
            "Epoch 97/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.0236 - val_accuracy: 0.9817\n",
            "Epoch 98/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9979 - val_loss: 0.0198 - val_accuracy: 0.9911\n",
            "Epoch 99/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9981 - val_loss: 0.0203 - val_accuracy: 0.9899\n",
            "Epoch 100/100\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.0203 - val_accuracy: 0.9899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYONptYpwiUW"
      },
      "source": [
        "Now, let's examine the performance of our model on the validation data after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrYOPIsJl_Mf",
        "outputId": "20b8ed17-6fc7-47e6-fa44-2f18b4101567"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ywxzJbUmMVI"
      },
      "source": [
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(loss_values) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "YbQsH32omBiv",
        "outputId": "ee021c12-ed39-40b0-d610-e9950f912bf8"
      },
      "source": [
        "plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf3UlEQVR4nO3de5wWdd3/8dcbEJCjnKzktFioochpgdRUKOun6Q2lUnBTipYHMkkqD6W3mkla+St/lFZWailJZsWNiVqaiuVdchAVFO8IQddTQAoYnoDP74+ZhYtlj+zOXrvXvJ+Px/XYme/MNfOZGbjeO4f9XooIzMwsv9oUuwAzMysuB4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8CahKS7JZ3a1PMWk6Q1ko7JYLkPSvpcOjxV0h/qM+8erGeApNcltd3TWi0fHAQ5ln5IVL62S3qjYHxqQ5YVEcdFxM+bet6WSNJFkhZW095b0tuSDqnvsiJiTkR8tInq2iW4IuK5iOgSEduaYvlV1hWS3tfUy7XicBDkWPoh0SUiugDPAf9R0Dancj5J7YpXZYt0K3C4pEFV2icDT0bE8iLUZLbHHAS2G0njJFVIulDSy8BNknpI+r2kdZJeTYf7Fbyn8HLHNEl/lnRNOu+zko7bw3kHSVooabOk+yRdJ+nWGuquT43fkPSXdHl/kNS7YPpnJK2VtEHSxTXtn4ioAP4EfKbKpFOAX9RVR5Wap0n6c8H4RyStlLRR0g8AFUx7r6Q/pfWtlzRH0j7ptFuAAcCd6RndBZLK0t/c26Xz7CdpvqR/SVol6YyCZV8u6XZJv0j3zQpJ5TXtg5pI6p4uY126Ly+R1Cad9j5JD6Xbtl7Sr9J2SfqepH9K2iTpyYacVVnjOQisJu8GegIDgTNJ/q3clI4PAN4AflDL+8cCzwC9gW8DP5OkPZj3l8CjQC/gcnb/8C1Unxr/EzgN2BdoD3wFQNIQ4Ifp8vdL11fth3fq54W1SDoQGJ7W29B9VbmM3sBvgUtI9sU/gCMKZwGuSut7P9CfZJ8QEZ9h17O6b1ezirlARfr+k4FvSvpQwfQJ6Tz7APPrU3M1vg90B/YHjiYJx9PSad8A/gD0INm330/bPwocBRyQvveTwIY9WLftqYjwyy+ANcAx6fA44G2gYy3zDwdeLRh/EPhcOjwNWFUwrRMQwLsbMi/Jh+hWoFPB9FuBW+u5TdXVeEnB+OeBe9LhS4G5BdM6p/vgmBqW3QnYBByejs8C/nsP99Wf0+FTgL8WzCeSD+7P1bDcjwOPVXcM0/GydF+2IwmNbUDXgulXATenw5cD9xVMGwK8Ucu+DeB9VdrapvtsSEHbWcCD6fAvgBuAflXe9yHgf4EPAG2K/X8hjy+fEVhN1kXEm5UjkjpJ+nF6ur8JWAjso5qfSHm5ciAitqSDXRo4737AvwraAJ6vqeB61vhywfCWgpr2K1x2RPybWn4rTWv6NXBKevYyleSDbk/2VaWqNUThuKR3SZor6YV0ubeSnDnUR+W+3FzQthboWzBedd90VMPuD/UG9kqXW906LiAJt0fTS0+nA0TEn0jOPq4D/inpBkndGrBeayQHgdWkare0XwYOBMZGRDeSU3kouIadgZeAnpI6FbT1r2X+xtT4UuGy03X2quM9Pye5jPERoCtwZyPrqFqD2HV7v0lyXIamy/10lWXW1pXwiyT7smtB2wDghTpqaoj1wDskl8R2W0dEvBwRZ0TEfiRnCtcrffIoImZHxCiSM5EDgPObsC6rg4PA6qsrybXu1yT1BC7LeoURsRZYDFwuqb2kw4D/yKjGO4ATJH1QUnvgCur+//Ew8BrJ5Y65EfF2I+u4CzhY0onpb+IzSC6RVeoKvA5slNSX3T8sXyG5Nr+biHgeeAS4SlJHSYcCnyU5q9hT7dNldZTUMW27HZglqaukgcCXKtchaVLBTfNXSYJru6TRksZK2gv4N/AmsL0RdVkDOQisvq4F9ib5re+vwD3NtN6pwGEkl2muBH4FvFXDvHtcY0SsAM4hudn7EskHVUUd7wmSy0ED05+NqiMi1gOTgKtJtncw8JeCWb4OjAQ2koTGb6ss4irgEkmvSfpKNauYQnLf4EXgd8BlEXFffWqrwQqSwKt8nQacS/Jhvhr4M8n+vDGdfzTwN0mvk9yM/mJErAa6AT8h2edrSbb9O42oyxpI6c0as1YhfeRwZURkfkZilhc+I7AWLb1s8F5JbSQdC0wE5hW7LrNS4r8YtZbu3SSXQHqRXKqZHhGPFbcks9LiS0NmZjnnS0NmZjnX6i4N9e7dO8rKyopdhplZq7JkyZL1EdGnummtLgjKyspYvHhxscswM2tVJK2taZovDZmZ5ZyDwMws5xwEZmY51+ruEZhZ83jnnXeoqKjgzTffrHtmazE6duxIv3792Guvver9HgeBmVWroqKCrl27UlZWRs3fKWQtSUSwYcMGKioqGDSo6jep1iwXl4bmzIGyMmjTJvk5Z05d7zCzN998k169ejkEWhFJ9OrVq8FncSV/RjBnDpx5JmxJv9pk7dpkHGDq1OLVZdYaOARanz05ZiV/RnDxxTtDoNKWLUm7mZnlIAiee65h7WbWMmzYsIHhw4czfPhw3v3ud9O3b98d42+//Xat7128eDEzZsyocx2HH354k9T64IMPcsIJJzTJsoqh5INgwICGtZvZnmnqe3G9evVi2bJlLFu2jLPPPpuZM2fuGG/fvj1bt26t8b3l5eXMnj27znU88sgjjSuyRJR8EMyaBZ067drWqVPSbmZNo/Je3Nq1ELHzXlxTP5gxbdo0zj77bMaOHcsFF1zAo48+ymGHHcaIESM4/PDDeeaZZ4Bdf0O//PLLOf300xk3bhz777//LgHRpUuXHfOPGzeOk08+mYMOOoipU6dS2TPzggULOOiggxg1ahQzZsxo0G/+t912G0OHDuWQQw7hwgsvBGDbtm1MmzaNQw45hKFDh/K9730PgNmzZzNkyBAOPfRQJk+e3Pid1QAlf7O48obwxRcnl4MGDEhCwDeKzZpObffimvr/WkVFBY888ght27Zl06ZNPPzww7Rr14777ruPr33ta/zmN7/Z7T0rV67kgQceYPPmzRx44IFMnz59t+fsH3vsMVasWMF+++3HEUccwV/+8hfKy8s566yzWLhwIYMGDWLKlCn1rvPFF1/kwgsvZMmSJfTo0YOPfvSjzJs3j/79+/PCCy+wfPlyAF577TUArr76ap599lk6dOiwo625lPwZAST/ENesge3bk58OAbOm1Zz34iZNmkTbtm0B2LhxI5MmTeKQQw5h5syZrFixotr3HH/88XTo0IHevXuz77778sorr+w2z5gxY+jXrx9t2rRh+PDhrFmzhpUrV7L//vvveCa/IUGwaNEixo0bR58+fWjXrh1Tp05l4cKF7L///qxevZpzzz2Xe+65h27dugFw6KGHMnXqVG699VbatWve39FzEQRmlq3mvBfXuXPnHcP/9V//xfjx41m+fDl33nlnjc/Pd+jQYcdw27Ztq72/UJ95mkKPHj14/PHHGTduHD/60Y/43Oc+B8Bdd93FOeecw9KlSxk9enRm66+Og8DMGq1Y9+I2btxI3759Abj55pubfPkHHnggq1evZs2aNQD86le/qvd7x4wZw0MPPcT69evZtm0bt912G0cffTTr169n+/btnHTSSVx55ZUsXbqU7du38/zzzzN+/Hi+9a1vsXHjRl5//fUm356alPw9AjPLXrHuxV1wwQWceuqpXHnllRx//PFNvvy9996b66+/nmOPPZbOnTszevToGue9//776dev347xX//611x99dWMHz+eiOD4449n4sSJPP7445x22mls374dgKuuuopt27bx6U9/mo0bNxIRzJgxg3322afJt6cmre47i8vLy8NfTGOWvaeffpr3v//9xS6j6F5//XW6dOlCRHDOOecwePBgZs6cWeyyalXdsZO0JCLKq5vfl4bMzGrxk5/8hOHDh3PwwQezceNGzjrrrGKX1OR8acjMrBYzZ85s8WcAjeUzAjOznHMQmJnlnIPAzCznHARmZjnnIDCzFmn8+PHce++9u7Rde+21TJ8+vcb3jBs3jsrHyz/2sY9V22fP5ZdfzjXXXFPruufNm8dTTz21Y/zSSy/lvvvua0j51Wqp3VU7CMysRZoyZQpz587dpW3u3Ln17u9nwYIFe/xHWVWD4IorruCYY47Zo2W1Bg4CM2uRTj75ZO66664dX0KzZs0aXnzxRY488kimT59OeXk5Bx98MJdddlm17y8rK2P9+vUAzJo1iwMOOIAPfvCDO7qqhuRvBEaPHs2wYcM46aST2LJlC4888gjz58/n/PPPZ/jw4fzjH/9g2rRp3HHHHUDyF8QjRoxg6NChnH766bz11ls71nfZZZcxcuRIhg4dysqVK+u9rcXurtp/R2BmdTrvPFi2rGmXOXw4XHttzdN79uzJmDFjuPvuu5k4cSJz587lk5/8JJKYNWsWPXv2ZNu2bXz4wx/miSee4NBDD612OUuWLGHu3LksW7aMrVu3MnLkSEaNGgXAiSeeyBlnnAHAJZdcws9+9jPOPfdcJkyYwAknnMDJJ5+8y7LefPNNpk2bxv33388BBxzAKaecwg9/+EPOO+88AHr37s3SpUu5/vrrueaaa/jpT39a535oCd1V+4zAzFqswstDhZeFbr/9dkaOHMmIESNYsWLFLpdxqnr44Yf5xCc+QadOnejWrRsTJkzYMW358uUceeSRDB06lDlz5tTYjXWlZ555hkGDBnHAAQcAcOqpp7Jw4cId00888UQARo0ataOjurq0hO6qfUZgZnWq7Tf3LE2cOJGZM2eydOlStmzZwqhRo3j22We55pprWLRoET169GDatGk1dj9dl2nTpjFv3jyGDRvGzTffzIMPPtioeiu7sm6Kbqwru6u+9957+dGPfsTtt9/OjTfeyF133cXChQu58847mTVrFk8++WSjAyHTMwJJx0p6RtIqSRfVMt9JkkJStR0imVk+denShfHjx3P66afvOBvYtGkTnTt3pnv37rzyyivcfffdtS7jqKOOYt68ebzxxhts3ryZO++8c8e0zZs38573vId33nmHOQXfq9m1a1c2b96827IOPPBA1qxZw6pVqwC45ZZbOProoxu1jS2hu+rMzggktQWuAz4CVACLJM2PiKeqzNcV+CLwt6xqMbPWa8qUKXziE5/YcYlo2LBhjBgxgoMOOoj+/ftzxBFH1Pr+kSNH8qlPfYphw4ax77777tKV9De+8Q3Gjh1Lnz59GDt27I4P/8mTJ3PGGWcwe/bsHTeJATp27MhNN93EpEmT2Lp1K6NHj+bss89u0Pa0xO6qM+uGWtJhwOUR8X/S8a8CRMRVVea7FvgjcD7wlYiotY9pd0Nt1jzcDXXr1ZK6oe4LPF8wXpG2FRY2EugfEXfVtiBJZ0paLGnxunXrmr5SM7McK9pTQ5LaAN8FvlzXvBFxQ0SUR0R5nz59si/OzCxHsgyCF4D+BeP90rZKXYFDgAclrQE+AMz3DWOzlqO1fYOh7dkxyzIIFgGDJQ2S1B6YDMyvnBgRGyOid0SURUQZ8FdgQl33CMyseXTs2JENGzY4DFqRiGDDhg107NixQe/L7KmhiNgq6QvAvUBb4MaIWCHpCmBxRMyvfQlmVkz9+vWjoqIC35drXTp27LjLU0n14S+vNzPLAX95vZmZ1chBYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznMg0CScdKekbSKkkXVTP9bElPSlom6c+ShmRZj5mZ7S6zIJDUFrgOOA4YAkyp5oP+lxExNCKGA98GvptVPWZmVr0szwjGAKsiYnVEvA3MBSYWzhARmwpGOwORYT1mZlaNdhkuuy/wfMF4BTC26kySzgG+BLQHPlTdgiSdCZwJMGDAgCYv1Mwsz4p+szgirouI9wIXApfUMM8NEVEeEeV9+vRp3gLNzEpclkHwAtC/YLxf2laTucDHM6zHzMyqkWUQLAIGSxokqT0wGZhfOIOkwQWjxwN/z7AeMzOrRmb3CCJiq6QvAPcCbYEbI2KFpCuAxRExH/iCpGOAd4BXgVOzqsfMzKqX5c1iImIBsKBK26UFw1/Mcv1mZla3ot8sNjOz4nIQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcq5eQSCps6Q26fABkiZI2ivb0szMrDnU94xgIdBRUl/gD8BngJuzKsrMzJpPfYNAEbEFOBG4PiImAQdnV5aZmTWXegeBpMOAqcBdaVvbbEoyM7PmVN8gOA/4KvC79Avo9wceyK4sMzNrLvX68vqIeAh4CCC9abw+ImZkWZiZmTWP+j419EtJ3SR1BpYDT0k6P9vSzMysOdT30tCQiNgEfBy4GxhE8uSQmZm1cvUNgr3Svxv4ODA/It4BIruyzMysudQ3CH4MrAE6AwslDQQ2ZVWUmZk1n/reLJ4NzC5oWitpfDYlmZlZc6rvzeLukr4raXH6+r8kZwdmZtbK1ffS0I3AZuCT6WsTcFNWRZmZWfOp16Uh4L0RcVLB+NclLcuiIDMza171PSN4Q9IHK0ckHQG8kU1JZmbWnOp7RnA28AtJ3dPxV4FTsynJzMyaU32fGnocGCapWzq+SdJ5wBNZFmdmZtlr0DeURcSm9C+MAb6UQT1mZtbMGvNVlWqyKszMrGgaEwTuYsLMrATUeo9A0maq/8AXsHcmFZmZWbOq9YwgIrpGRLdqXl0jos4bzZKOlfSMpFWSLqpm+pckPSXpCUn3p30YmZlZM2rMpaFaSWoLXAccBwwBpkgaUmW2x4DyiDgUuAP4dlb1mJlZ9TILAmAMsCoiVkfE28BcYGLhDBHxQERsSUf/CvTLsB4zM6tGlkHQF3i+YLwibavJZ0m+9GY3ks6s7PBu3bp1TViimZllGQT1JunTQDnwneqmR8QNEVEeEeV9+vRp3uLMzEpcfbuY2BMvAP0LxvulbbuQdAxwMXB0RLyVYT1mZlaNLM8IFgGDJQ2S1B6YDMwvnEHSCJJvP5sQEf/MsBYzM6tBZkEQEVuBLwD3Ak8Dt0fECklXSJqQzvYdoAvwa0nLJM2vYXFmZpaRLC8NERELgAVV2i4tGD4my/WbmVndWsTNYjMzKx4HgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OcyzQIJB0r6RlJqyRdVM30oyQtlbRV0slZ1mJmZtXLLAgktQWuA44DhgBTJA2pMttzwDTgl1nVYWZmtWuX4bLHAKsiYjWApLnAROCpyhkiYk06bXuGdZiZWS2yvDTUF3i+YLwibWswSWdKWixp8bp165qkODMzS7SKm8URcUNElEdEeZ8+fYpdjplZSckyCF4A+heM90vbzMysBckyCBYBgyUNktQemAzMz3B9Zma2BzILgojYCnwBuBd4Grg9IlZIukLSBABJoyVVAJOAH0takVU9ZmZWvSyfGiIiFgALqrRdWjC8iOSSkZmZFUmruFlsZmbZcRCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeVc7oJgzhwoK4M2bZKfc+YUuyIzs+LKVRDMmQNnnglr10JE8vMznwEpCYXPf35nSPTunbyqDmcdHg6qls/HqDREwNNPwxtv7D5t+/bmq2PzZli+PKmnaCKiVb1GjRoVe+KhhyK6dYtIdnfjXlLys1ev5CU1zXDhsmtb18CBEdOnJz+bat1NOdzS62tMrfU9Rq11X/bsGdG5c7I9++zTMmvt2XPX/Q0RPXpEdO++s+6ePXduT48eSXv//jvrg4h27XYew/btI7p2TYb32mtne7du1dfRo8fO+bt3T8Ybso8ray2soUOHiBNPrHv/DRwYceutDf8MBBbX9LmqKGoMNVx5eXksXry4we+79lqYOTODgszMmlmnTnDDDTB1av3fI2lJRJRXNy03l4bOOw8GDCh2FWZmjbdlC1x8cdMtLzdBAPDNbyZJambW2j33XNMtK1dBMHVqcjo1cGAyLhW3HjOzPdWUVzhyFQSQhMGaNcntmVtuSUJBSn5On75zvFev5FU4DM0THpXrcFC1XD5GVkydOsGsWU23vNwFQaHKUNi+Pfl5/fU7x9evT16Fw1XDo7qwaOzwwIHJOmpbV31Cq5jDLb2+xtZan2PkfdkyairF+gYObPiN4rrk5qkhM7M8K9pTQ5KOlfSMpFWSLqpmegdJv0qn/01SWZb1mJnZ7jILAkltgeuA44AhwBRJQ6rM9lng1Yh4H/A94FtZ1WNmZtXL8oxgDLAqIlZHxNvAXGBilXkmAj9Ph+8APiz59puZWXPKMgj6As8XjFekbdXOExFbgY1Ar6oLknSmpMWSFq9bty6jcs3M8qlVPDUUETdERHlElPfp06fY5ZiZlZR2GS77BaB/wXi/tK26eSoktQO6AxtqW+iSJUvWS1rbgDp6A+sbMH+pyON253GbIZ/bncdthsZt98CaJmQZBIuAwZIGkXzgTwb+s8o884FTgf8BTgb+FHU8zxoRDTolkLS4pkemSlketzuP2wz53O48bjNkt92ZBUFEbJX0BeBeoC1wY0SskHQFSXeo84GfAbdIWgX8iyQszMysGWV5RkBELAAWVGm7tGD4TWBSljWYmVntWsXN4ka6odgFFEketzuP2wz53O48bjNktN2trosJMzNrWnk4IzAzs1o4CMzMcq6kg6CuTu9KgaT+kh6Q9JSkFZK+mLb3lPRHSX9Pf/Yodq1NTVJbSY9J+n06PijtvHBV2plh+2LX2NQk7SPpDkkrJT0t6bCcHOuZ6b/v5ZJuk9Sx1I63pBsl/VPS8oK2ao+tErPTbX9C0sjGrLtkg6Cend6Vgq3AlyNiCPAB4Jx0Oy8C7o+IwcD96Xip+SLwdMH4t4DvpZ0YvkrSqWGp+X/APRFxEDCMZPtL+lhL6gvMAMoj4hCSx9EnU3rH+2bg2CptNR3b44DB6etM4IeNWXHJBgH16/Su1YuIlyJiaTq8meSDoS+7duj3c+DjxakwG5L6AccDP03HBXyIpPNCKM1t7g4cRfL3N0TE2xHxGiV+rFPtgL3THgg6AS9RYsc7IhaS/D1VoZqO7UTgF5H4K7CPpPfs6bpLOQjq0+ldSUm/z2EE8DfgXRHxUjrpZeBdRSorK9cCFwDb0/FewGtp54VQmsd7ELAOuCm9JPZTSZ0p8WMdES8A1wDPkQTARmAJpX+8oeZj26Sfb6UcBLkiqQvwG+C8iNhUOC3ttqNknhOWdALwz4hYUuxamlk7YCTww4gYAfybKpeBSu1YA6TXxSeSBOF+QGd2v4RS8rI8tqUcBPXp9K4kSNqLJATmRMRv0+ZXKk8V05//LFZ9GTgCmCBpDcklvw+RXDvfJ710AKV5vCuAioj4Wzp+B0kwlPKxBjgGeDYi1kXEO8BvSf4NlPrxhpqPbZN+vpVyEOzo9C59mmAySSd3JSW9Nv4z4OmI+G7BpMoO/Uh//ndz15aViPhqRPSLiDKS4/qniJgKPEDSeSGU2DYDRMTLwPOSDkybPgw8RQkf69RzwAckdUr/vVdud0kf71RNx3Y+cEr69NAHgI0Fl5AaLiJK9gV8DPhf4B/AxcWuJ6Nt/CDJ6eITwLL09TGSa+b3A38H7gN6FrvWjLZ/HPD7dHh/4FFgFfBroEOx68tge4cDi9PjPQ/okYdjDXwdWAksB24BOpTa8QZuI7kH8g7J2d9nazq2gEieivwH8CTJE1V7vG53MWFmlnOlfGnIzMzqwUFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZilJ2yQtK3g1WedtksoKe5U0a0ky/c5is1bmjYgYXuwizJqbzwjM6iBpjaRvS3pS0qOS3pe2l0n6U9of/P2SBqTt75L0O0mPp6/D00W1lfSTtF/9P0jaO51/Rvp9Ek9ImlukzbQccxCY7bR3lUtDnyqYtjEihgI/IOn5FOD7wM8j4lBgDjA7bZ8NPBQRw0j6AlqRtg8GrouIg4HXgJPS9ouAEelyzs5q48xq4r8sNktJej0iulTTvgb4UESsTjv4ezkieklaD7wnIt5J21+KiN6S1gH9IuKtgmWUAX+M5AtGkHQhsFdEXCnpHuB1ki4j5kXE6xlvqtkufEZgVj9Rw3BDvFUwvI2d9+iOJ+k3ZiSwqKBHTbNm4SAwq59PFfz8n3T4EZLeTwGmAg+nw/cD02HH9yp3r2mhktoA/SPiAeBCoDuw21mJWZb8m4fZTntLWlYwfk9EVD5C2kPSEyS/1U9J284l+baw80m+Oey0tP2LwA2SPkvym/90kl4lq9MWuDUNCwGzI/n6SbNm43sEZnVI7xGUR8T6YtdilgVfGjIzyzmfEZiZ5ZzPCMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOf+P0VNKD+xU2SIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cyQlXcNKmqrA",
        "outputId": "c0cbf5af-b167-4192-f227-b40d61142ea1"
      },
      "source": [
        "plt.clf()\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.xlim(10, 100)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuHD96l0w5-n"
      },
      "source": [
        "The model starts somewhat overfitting after 40 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hWisEcC2Aym"
      },
      "source": [
        "# Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFBorkCyUsn"
      },
      "source": [
        "Now, let's retrain our model from scratch for 40 epochs only on all of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz_OQul5nn__",
        "outputId": "fe1dcdbd-c29d-406e-d504-3f297b673b72"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
        "model.add(layers.Dense(8, activation='relu'))\n",
        "model.add(layers.Dense(6, activation='relu'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=512,\n",
        "    epochs=40\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "388/388 [==============================] - 2s 2ms/step - loss: 4.0978 - accuracy: 0.6942\n",
            "Epoch 2/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0213 - accuracy: 0.9973\n",
            "Epoch 3/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9982\n",
            "Epoch 4/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9982\n",
            "Epoch 5/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9982\n",
            "Epoch 6/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9981\n",
            "Epoch 7/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9979\n",
            "Epoch 8/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9981\n",
            "Epoch 9/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9983\n",
            "Epoch 10/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9981\n",
            "Epoch 11/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9982\n",
            "Epoch 12/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9982\n",
            "Epoch 13/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9982\n",
            "Epoch 14/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9982\n",
            "Epoch 15/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9984\n",
            "Epoch 16/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9984\n",
            "Epoch 17/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9981\n",
            "Epoch 18/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9982\n",
            "Epoch 19/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9984\n",
            "Epoch 20/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9981\n",
            "Epoch 21/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9978\n",
            "Epoch 22/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9981\n",
            "Epoch 23/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9979\n",
            "Epoch 24/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9975\n",
            "Epoch 25/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9981\n",
            "Epoch 26/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9978\n",
            "Epoch 27/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9980\n",
            "Epoch 28/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9981\n",
            "Epoch 29/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0152 - accuracy: 0.9980\n",
            "Epoch 30/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9980\n",
            "Epoch 31/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9982\n",
            "Epoch 32/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9983\n",
            "Epoch 33/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9980\n",
            "Epoch 34/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9982\n",
            "Epoch 35/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9979\n",
            "Epoch 36/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9983\n",
            "Epoch 37/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9983\n",
            "Epoch 38/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9981\n",
            "Epoch 39/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9982\n",
            "Epoch 40/40\n",
            "388/388 [==============================] - 1s 2ms/step - loss: 0.0142 - accuracy: 0.9985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAZBqabsphMx",
        "outputId": "4f1010ff-79df-46ae-a5ef-3de6d6bb2eec"
      },
      "source": [
        "result = model.evaluate(x_test, y_test)\n",
        "print(\"Model Accuracy for Test Data: {acc:.2f}%\".format(acc=result[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2660/2660 [==============================] - 3s 1ms/step - loss: 0.0135 - accuracy: 0.9987\n",
            "Model Accuracy for Test Data: 99.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f_mIivz15-P"
      },
      "source": [
        "# Another Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXdD0xPv6fU-"
      },
      "source": [
        "Although the model is performing well; however, there are some problems in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT_Qmf7h626W"
      },
      "source": [
        "First of all, we can improve the overfitting issue by reducing the size of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx6ovaFk7LiN"
      },
      "source": [
        "Second, the last layer should have the activation function of \"sigmoid\" as we want our model to output a prediction value (between 0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZmnNW0j19gG",
        "outputId": "401bc152-82a7-447c-c3ea-8bec4f75dafb"
      },
      "source": [
        "model2 = models.Sequential()\n",
        "model2.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
        "model2.add(layers.Dense(2, activation='relu'))\n",
        "model2.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model2.fit(\n",
        "    x_train_partial,\n",
        "    y_train_partial,\n",
        "    batch_size=512,\n",
        "    epochs=1000,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "278/278 [==============================] - 2s 3ms/step - loss: 0.2101 - accuracy: 0.9527 - val_loss: 0.1076 - val_accuracy: 0.9979\n",
            "Epoch 2/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9987 - val_loss: 0.0651 - val_accuracy: 0.9988\n",
            "Epoch 3/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0451 - accuracy: 0.9987 - val_loss: 0.0410 - val_accuracy: 0.9988\n",
            "Epoch 4/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0431 - accuracy: 0.9987 - val_loss: 0.0271 - val_accuracy: 0.9988\n",
            "Epoch 5/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0300 - accuracy: 0.9989 - val_loss: 0.0183 - val_accuracy: 0.9988\n",
            "Epoch 6/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0312 - accuracy: 0.9987 - val_loss: 0.0133 - val_accuracy: 0.9989\n",
            "Epoch 7/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0130 - accuracy: 0.9989 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 8/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.0090 - val_accuracy: 0.9989\n",
            "Epoch 9/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9987 - val_loss: 0.0081 - val_accuracy: 0.9989\n",
            "Epoch 10/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0079 - val_accuracy: 0.9988\n",
            "Epoch 11/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.0079 - val_accuracy: 0.9987\n",
            "Epoch 12/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0078 - val_accuracy: 0.9988\n",
            "Epoch 13/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.0079 - val_accuracy: 0.9988\n",
            "Epoch 14/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.0081 - val_accuracy: 0.9988\n",
            "Epoch 15/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 0.0083 - val_accuracy: 0.9988\n",
            "Epoch 16/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0084 - val_accuracy: 0.9987\n",
            "Epoch 17/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.0085 - val_accuracy: 0.9987\n",
            "Epoch 18/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.0086 - val_accuracy: 0.9987\n",
            "Epoch 19/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.0089 - val_accuracy: 0.9987\n",
            "Epoch 20/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0095 - val_accuracy: 0.9987\n",
            "Epoch 21/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0092 - val_accuracy: 0.9987\n",
            "Epoch 22/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.0087 - val_accuracy: 0.9986\n",
            "Epoch 23/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0094 - val_accuracy: 0.9986\n",
            "Epoch 24/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0083 - val_accuracy: 0.9986\n",
            "Epoch 25/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0082 - val_accuracy: 0.9987\n",
            "Epoch 26/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0088 - val_accuracy: 0.9987\n",
            "Epoch 27/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0087 - val_accuracy: 0.9986\n",
            "Epoch 28/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0082 - val_accuracy: 0.9987\n",
            "Epoch 29/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0078 - val_accuracy: 0.9987\n",
            "Epoch 30/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0075 - val_accuracy: 0.9987\n",
            "Epoch 31/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0072 - val_accuracy: 0.9987\n",
            "Epoch 32/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0071 - val_accuracy: 0.9987\n",
            "Epoch 33/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0070 - val_accuracy: 0.9987\n",
            "Epoch 34/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0066 - val_accuracy: 0.9988\n",
            "Epoch 35/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0065 - val_accuracy: 0.9988\n",
            "Epoch 36/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0064 - val_accuracy: 0.9988\n",
            "Epoch 37/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0064 - val_accuracy: 0.9988\n",
            "Epoch 38/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0061 - val_accuracy: 0.9988\n",
            "Epoch 39/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0060 - val_accuracy: 0.9989\n",
            "Epoch 40/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0057 - val_accuracy: 0.9990\n",
            "Epoch 41/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
            "Epoch 42/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0055 - val_accuracy: 0.9989\n",
            "Epoch 43/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
            "Epoch 44/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
            "Epoch 45/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0055 - val_accuracy: 0.9989\n",
            "Epoch 46/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0052 - val_accuracy: 0.9989\n",
            "Epoch 47/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0051 - val_accuracy: 0.9990\n",
            "Epoch 48/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0054 - val_accuracy: 0.9992\n",
            "Epoch 49/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 50/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0057 - val_accuracy: 0.9992\n",
            "Epoch 51/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 52/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 53/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 54/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 55/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 56/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9993\n",
            "Epoch 57/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 58/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 59/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
            "Epoch 60/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 61/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 62/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9990\n",
            "Epoch 63/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 64/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0051 - val_accuracy: 0.9992\n",
            "Epoch 65/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 66/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 67/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 68/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 69/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 70/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 71/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 72/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9992\n",
            "Epoch 73/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 74/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 75/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 76/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 77/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 78/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 79/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 80/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 81/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 82/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 83/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 84/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 85/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0052 - val_accuracy: 0.9993\n",
            "Epoch 86/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 87/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 88/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
            "Epoch 89/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 90/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 91/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 92/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 93/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 94/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 95/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 96/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 97/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 98/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 99/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9993\n",
            "Epoch 100/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 101/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 102/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 103/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 104/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 105/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 106/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 107/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 108/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 109/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 110/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 111/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 112/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0051 - val_accuracy: 0.9992\n",
            "Epoch 113/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 114/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 115/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 116/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 117/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 118/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9991\n",
            "Epoch 119/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 120/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 121/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 122/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 123/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 124/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 125/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 126/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 127/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 128/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 129/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 130/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 131/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 132/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 133/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 134/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 135/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 136/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 137/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 138/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 139/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9993\n",
            "Epoch 140/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 141/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 142/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 143/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 144/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 145/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 146/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 147/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 148/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 149/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 150/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 151/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 152/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 153/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 154/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 155/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 156/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 157/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9993\n",
            "Epoch 158/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 159/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9993\n",
            "Epoch 160/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 161/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 162/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 163/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 164/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 165/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 166/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 167/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 168/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 169/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 170/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 171/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9993\n",
            "Epoch 172/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 173/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 174/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9990\n",
            "Epoch 175/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0041 - val_accuracy: 0.9993\n",
            "Epoch 176/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 177/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 178/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 179/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 180/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 181/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 182/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 183/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9993\n",
            "Epoch 184/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 185/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 186/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 187/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 188/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 189/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 190/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 191/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 192/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 193/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 194/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 195/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9993\n",
            "Epoch 196/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 197/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 198/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
            "Epoch 199/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 200/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 201/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 202/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 203/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 204/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 205/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 206/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 207/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 208/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 209/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 210/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 211/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 212/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 213/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 214/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 215/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 216/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 217/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9992\n",
            "Epoch 218/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9993\n",
            "Epoch 219/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\n",
            "Epoch 220/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 221/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 222/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 223/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 224/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9993\n",
            "Epoch 225/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9993\n",
            "Epoch 226/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 227/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9993\n",
            "Epoch 228/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9992\n",
            "Epoch 229/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0047 - val_accuracy: 0.9993\n",
            "Epoch 230/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0043 - val_accuracy: 0.9992\n",
            "Epoch 231/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 232/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 233/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 234/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 235/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Epoch 236/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 237/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0047 - val_accuracy: 0.9992\n",
            "Epoch 238/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 239/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 240/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 241/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 242/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0051 - val_accuracy: 0.9992\n",
            "Epoch 243/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 244/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 245/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 246/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0052 - val_accuracy: 0.9992\n",
            "Epoch 247/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 248/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 249/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 250/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
            "Epoch 251/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9991\n",
            "Epoch 252/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9991\n",
            "Epoch 253/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
            "Epoch 254/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 255/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0051 - val_accuracy: 0.9991\n",
            "Epoch 256/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 257/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0052 - val_accuracy: 0.9992\n",
            "Epoch 258/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9992\n",
            "Epoch 259/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0051 - val_accuracy: 0.9992\n",
            "Epoch 260/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 261/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9992\n",
            "Epoch 262/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9991\n",
            "Epoch 263/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 264/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 265/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 266/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 267/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0056 - val_accuracy: 0.9992\n",
            "Epoch 268/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0053 - val_accuracy: 0.9992\n",
            "Epoch 269/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 270/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 271/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9990\n",
            "Epoch 272/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 273/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 274/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0051 - val_accuracy: 0.9991\n",
            "Epoch 275/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 276/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 277/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0052 - val_accuracy: 0.9991\n",
            "Epoch 278/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 279/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 280/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9992\n",
            "Epoch 281/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 282/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 283/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
            "Epoch 284/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 285/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9992\n",
            "Epoch 286/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9992\n",
            "Epoch 287/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0055 - val_accuracy: 0.9991\n",
            "Epoch 288/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 289/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 290/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 291/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 292/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0055 - val_accuracy: 0.9991\n",
            "Epoch 293/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9991\n",
            "Epoch 294/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9991\n",
            "Epoch 295/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
            "Epoch 296/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 297/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 298/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 299/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 300/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9992\n",
            "Epoch 301/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9992\n",
            "Epoch 302/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9992\n",
            "Epoch 303/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 304/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9991\n",
            "Epoch 305/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 306/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9991\n",
            "Epoch 307/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 308/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0056 - val_accuracy: 0.9992\n",
            "Epoch 309/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
            "Epoch 310/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0057 - val_accuracy: 0.9990\n",
            "Epoch 311/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9991\n",
            "Epoch 312/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
            "Epoch 313/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
            "Epoch 314/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9991\n",
            "Epoch 315/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 316/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9990\n",
            "Epoch 317/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
            "Epoch 318/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
            "Epoch 319/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
            "Epoch 320/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 321/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 322/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 323/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9992\n",
            "Epoch 324/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
            "Epoch 325/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0059 - val_accuracy: 0.9991\n",
            "Epoch 326/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 327/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0060 - val_accuracy: 0.9990\n",
            "Epoch 328/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
            "Epoch 329/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0061 - val_accuracy: 0.9990\n",
            "Epoch 330/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0061 - val_accuracy: 0.9991\n",
            "Epoch 331/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 332/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 333/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
            "Epoch 334/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 335/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 336/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 337/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0061 - val_accuracy: 0.9991\n",
            "Epoch 338/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
            "Epoch 339/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0059 - val_accuracy: 0.9990\n",
            "Epoch 340/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0061 - val_accuracy: 0.9991\n",
            "Epoch 341/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9991\n",
            "Epoch 342/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
            "Epoch 343/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0059 - val_accuracy: 0.9991\n",
            "Epoch 344/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 345/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9990\n",
            "Epoch 346/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9990\n",
            "Epoch 347/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0058 - val_accuracy: 0.9990\n",
            "Epoch 348/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0057 - val_accuracy: 0.9991\n",
            "Epoch 349/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 350/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 351/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0067 - val_accuracy: 0.9990\n",
            "Epoch 352/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0063 - val_accuracy: 0.9990\n",
            "Epoch 353/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 354/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 355/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 356/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 357/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 358/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9989\n",
            "Epoch 359/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0063 - val_accuracy: 0.9990\n",
            "Epoch 360/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9990\n",
            "Epoch 361/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 362/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
            "Epoch 363/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 364/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 365/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0066 - val_accuracy: 0.9991\n",
            "Epoch 366/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 367/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0070 - val_accuracy: 0.9991\n",
            "Epoch 368/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0066 - val_accuracy: 0.9991\n",
            "Epoch 369/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0065 - val_accuracy: 0.9991\n",
            "Epoch 370/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9991\n",
            "Epoch 371/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9991\n",
            "Epoch 372/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
            "Epoch 373/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0063 - val_accuracy: 0.9991\n",
            "Epoch 374/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
            "Epoch 375/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
            "Epoch 376/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0065 - val_accuracy: 0.9990\n",
            "Epoch 377/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0065 - val_accuracy: 0.9990\n",
            "Epoch 378/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0061 - val_accuracy: 0.9991\n",
            "Epoch 379/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0061 - val_accuracy: 0.9990\n",
            "Epoch 380/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
            "Epoch 381/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
            "Epoch 382/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9990\n",
            "Epoch 383/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0065 - val_accuracy: 0.9991\n",
            "Epoch 384/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0063 - val_accuracy: 0.9990\n",
            "Epoch 385/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
            "Epoch 386/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0072 - val_accuracy: 0.9991\n",
            "Epoch 387/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0067 - val_accuracy: 0.9990\n",
            "Epoch 388/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0065 - val_accuracy: 0.9991\n",
            "Epoch 389/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 390/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 391/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 392/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
            "Epoch 393/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 394/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 395/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0066 - val_accuracy: 0.9991\n",
            "Epoch 396/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0067 - val_accuracy: 0.9991\n",
            "Epoch 397/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 398/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 399/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0067 - val_accuracy: 0.9991\n",
            "Epoch 400/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0067 - val_accuracy: 0.9991\n",
            "Epoch 401/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0069 - val_accuracy: 0.9991\n",
            "Epoch 402/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9990\n",
            "Epoch 403/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9991\n",
            "Epoch 404/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
            "Epoch 405/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 406/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 407/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 408/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 409/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9991\n",
            "Epoch 410/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 411/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0068 - val_accuracy: 0.9991\n",
            "Epoch 412/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 413/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 414/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0074 - val_accuracy: 0.9990\n",
            "Epoch 415/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 416/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
            "Epoch 417/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0068 - val_accuracy: 0.9991\n",
            "Epoch 418/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
            "Epoch 419/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 420/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 421/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0073 - val_accuracy: 0.9991\n",
            "Epoch 422/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9991\n",
            "Epoch 423/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0067 - val_accuracy: 0.9990\n",
            "Epoch 424/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 425/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 426/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 427/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 428/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 429/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0068 - val_accuracy: 0.9991\n",
            "Epoch 430/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0074 - val_accuracy: 0.9990\n",
            "Epoch 431/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 432/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0074 - val_accuracy: 0.9990\n",
            "Epoch 433/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
            "Epoch 434/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 435/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 436/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 437/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9990\n",
            "Epoch 438/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 439/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 440/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9991\n",
            "Epoch 441/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
            "Epoch 442/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 443/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 444/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0070 - val_accuracy: 0.9991\n",
            "Epoch 445/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 446/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 447/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 448/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 449/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 450/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9988\n",
            "Epoch 451/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 452/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 453/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 454/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 455/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 456/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0073 - val_accuracy: 0.9989\n",
            "Epoch 457/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0074 - val_accuracy: 0.9989\n",
            "Epoch 458/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9989\n",
            "Epoch 459/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9989\n",
            "Epoch 460/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 461/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0080 - val_accuracy: 0.9989\n",
            "Epoch 462/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9989\n",
            "Epoch 463/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0072 - val_accuracy: 0.9989\n",
            "Epoch 464/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 465/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 466/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 467/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
            "Epoch 468/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 469/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 470/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0071 - val_accuracy: 0.9990\n",
            "Epoch 471/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0076 - val_accuracy: 0.9991\n",
            "Epoch 472/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0072 - val_accuracy: 0.9991\n",
            "Epoch 473/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
            "Epoch 474/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
            "Epoch 475/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 476/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0076 - val_accuracy: 0.9991\n",
            "Epoch 477/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 478/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
            "Epoch 479/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0074 - val_accuracy: 0.9991\n",
            "Epoch 480/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
            "Epoch 481/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0075 - val_accuracy: 0.9991\n",
            "Epoch 482/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 483/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0074 - val_accuracy: 0.9990\n",
            "Epoch 484/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9989\n",
            "Epoch 485/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 486/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 487/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
            "Epoch 488/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 489/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 490/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 491/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 492/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 493/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 494/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 495/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 496/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
            "Epoch 497/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 498/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9990\n",
            "Epoch 499/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9990\n",
            "Epoch 500/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0088 - val_accuracy: 0.9990\n",
            "Epoch 501/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 502/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0077 - val_accuracy: 0.9991\n",
            "Epoch 503/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9991\n",
            "Epoch 504/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0076 - val_accuracy: 0.9992\n",
            "Epoch 505/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 506/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 507/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 508/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0080 - val_accuracy: 0.9991\n",
            "Epoch 509/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 510/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0078 - val_accuracy: 0.9989\n",
            "Epoch 511/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9990\n",
            "Epoch 512/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 513/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 514/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0120 - val_accuracy: 0.9990\n",
            "Epoch 515/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
            "Epoch 516/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9990\n",
            "Epoch 517/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 518/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 519/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 520/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9990\n",
            "Epoch 521/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
            "Epoch 522/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
            "Epoch 523/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9991\n",
            "Epoch 524/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 525/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
            "Epoch 526/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
            "Epoch 527/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 528/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9990\n",
            "Epoch 529/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 530/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 531/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 532/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 533/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
            "Epoch 534/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 535/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 536/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
            "Epoch 537/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
            "Epoch 538/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 539/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 540/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
            "Epoch 541/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 542/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 543/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 544/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
            "Epoch 545/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 546/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0082 - val_accuracy: 0.9990\n",
            "Epoch 547/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
            "Epoch 548/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0086 - val_accuracy: 0.9990\n",
            "Epoch 549/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0088 - val_accuracy: 0.9990\n",
            "Epoch 550/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0086 - val_accuracy: 0.9990\n",
            "Epoch 551/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0085 - val_accuracy: 0.9990\n",
            "Epoch 552/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 553/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 554/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
            "Epoch 555/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0085 - val_accuracy: 0.9990\n",
            "Epoch 556/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0089 - val_accuracy: 0.9989\n",
            "Epoch 557/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 558/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 559/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0091 - val_accuracy: 0.9990\n",
            "Epoch 560/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0093 - val_accuracy: 0.9990\n",
            "Epoch 561/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 562/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 563/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 564/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 565/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 566/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0084 - val_accuracy: 0.9991\n",
            "Epoch 567/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 568/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9990\n",
            "Epoch 569/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
            "Epoch 570/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 571/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 572/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9990\n",
            "Epoch 573/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 574/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 575/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 576/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 577/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 578/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 579/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 580/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 581/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 582/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 583/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 584/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
            "Epoch 585/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 586/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0088 - val_accuracy: 0.9990\n",
            "Epoch 587/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0096 - val_accuracy: 0.9989\n",
            "Epoch 588/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0098 - val_accuracy: 0.9989\n",
            "Epoch 589/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 590/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0087 - val_accuracy: 0.9990\n",
            "Epoch 591/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 592/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 593/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0094 - val_accuracy: 0.9990\n",
            "Epoch 594/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0091 - val_accuracy: 0.9990\n",
            "Epoch 595/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 596/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 597/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0095 - val_accuracy: 0.9990\n",
            "Epoch 598/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0091 - val_accuracy: 0.9990\n",
            "Epoch 599/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0095 - val_accuracy: 0.9990\n",
            "Epoch 600/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 601/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0091 - val_accuracy: 0.9990\n",
            "Epoch 602/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 603/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0088 - val_accuracy: 0.9990\n",
            "Epoch 604/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 605/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0101 - val_accuracy: 0.9989\n",
            "Epoch 606/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9990\n",
            "Epoch 607/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0094 - val_accuracy: 0.9990\n",
            "Epoch 608/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 609/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
            "Epoch 610/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0094 - val_accuracy: 0.9990\n",
            "Epoch 611/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0099 - val_accuracy: 0.9990\n",
            "Epoch 612/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
            "Epoch 613/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0099 - val_accuracy: 0.9989\n",
            "Epoch 614/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
            "Epoch 615/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0098 - val_accuracy: 0.9989\n",
            "Epoch 616/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
            "Epoch 617/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0103 - val_accuracy: 0.9990\n",
            "Epoch 618/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0102 - val_accuracy: 0.9989\n",
            "Epoch 619/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0095 - val_accuracy: 0.9990\n",
            "Epoch 620/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0093 - val_accuracy: 0.9990\n",
            "Epoch 621/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
            "Epoch 622/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0097 - val_accuracy: 0.9989\n",
            "Epoch 623/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0095 - val_accuracy: 0.9990\n",
            "Epoch 624/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0103 - val_accuracy: 0.9989\n",
            "Epoch 625/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 626/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
            "Epoch 627/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9989\n",
            "Epoch 628/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 629/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0099 - val_accuracy: 0.9989\n",
            "Epoch 630/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0102 - val_accuracy: 0.9988\n",
            "Epoch 631/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0106 - val_accuracy: 0.9985\n",
            "Epoch 632/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0099 - val_accuracy: 0.9990\n",
            "Epoch 633/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9990\n",
            "Epoch 634/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9988\n",
            "Epoch 635/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0099 - val_accuracy: 0.9989\n",
            "Epoch 636/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9990\n",
            "Epoch 637/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0103 - val_accuracy: 0.9989\n",
            "Epoch 638/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0098 - val_accuracy: 0.9989\n",
            "Epoch 639/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 640/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9988\n",
            "Epoch 641/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
            "Epoch 642/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0098 - val_accuracy: 0.9989\n",
            "Epoch 643/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
            "Epoch 644/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 645/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 646/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0094 - val_accuracy: 0.9990\n",
            "Epoch 647/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0099 - val_accuracy: 0.9989\n",
            "Epoch 648/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9989\n",
            "Epoch 649/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9989\n",
            "Epoch 650/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0102 - val_accuracy: 0.9989\n",
            "Epoch 651/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9988\n",
            "Epoch 652/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
            "Epoch 653/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0098 - val_accuracy: 0.9989\n",
            "Epoch 654/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0100 - val_accuracy: 0.9990\n",
            "Epoch 655/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9990\n",
            "Epoch 656/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
            "Epoch 657/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0098 - val_accuracy: 0.9990\n",
            "Epoch 658/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0096 - val_accuracy: 0.9991\n",
            "Epoch 659/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0103 - val_accuracy: 0.9989\n",
            "Epoch 660/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9989\n",
            "Epoch 661/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 662/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9989\n",
            "Epoch 663/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 664/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
            "Epoch 665/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 666/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 667/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9989\n",
            "Epoch 668/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 669/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 670/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 671/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 672/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 673/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0108 - val_accuracy: 0.9989\n",
            "Epoch 674/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 675/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9988\n",
            "Epoch 676/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9988\n",
            "Epoch 677/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 678/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 679/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 680/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0116 - val_accuracy: 0.9988\n",
            "Epoch 681/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9988\n",
            "Epoch 682/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9989\n",
            "Epoch 683/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 684/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9989\n",
            "Epoch 685/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 686/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 687/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 688/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 689/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 690/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 691/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 692/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9988\n",
            "Epoch 693/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 694/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 695/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 696/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0122 - val_accuracy: 0.9990\n",
            "Epoch 697/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0108 - val_accuracy: 0.9989\n",
            "Epoch 698/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 699/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0124 - val_accuracy: 0.9988\n",
            "Epoch 700/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9989\n",
            "Epoch 701/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 702/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 703/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0125 - val_accuracy: 0.9985\n",
            "Epoch 704/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9989\n",
            "Epoch 705/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0115 - val_accuracy: 0.9989\n",
            "Epoch 706/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9988\n",
            "Epoch 707/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0121 - val_accuracy: 0.9988\n",
            "Epoch 708/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 709/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 710/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 711/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 712/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 713/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 714/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 715/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0098 - val_accuracy: 0.9991\n",
            "Epoch 716/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0100 - val_accuracy: 0.9990\n",
            "Epoch 717/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9990\n",
            "Epoch 718/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9988\n",
            "Epoch 719/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 720/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 721/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 722/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9989\n",
            "Epoch 723/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9989\n",
            "Epoch 724/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9988\n",
            "Epoch 725/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 726/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0119 - val_accuracy: 0.9989\n",
            "Epoch 727/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0124 - val_accuracy: 0.9988\n",
            "Epoch 728/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 729/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9988\n",
            "Epoch 730/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0119 - val_accuracy: 0.9988\n",
            "Epoch 731/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9988\n",
            "Epoch 732/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 733/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0123 - val_accuracy: 0.9988\n",
            "Epoch 734/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0115 - val_accuracy: 0.9989\n",
            "Epoch 735/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 736/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0119 - val_accuracy: 0.9989\n",
            "Epoch 737/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 738/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 739/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 740/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 741/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9989\n",
            "Epoch 742/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 743/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 744/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 745/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
            "Epoch 746/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 747/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0123 - val_accuracy: 0.9987\n",
            "Epoch 748/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 749/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0119 - val_accuracy: 0.9989\n",
            "Epoch 750/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
            "Epoch 751/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9989\n",
            "Epoch 752/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 753/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9990\n",
            "Epoch 754/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9990\n",
            "Epoch 755/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0111 - val_accuracy: 0.9989\n",
            "Epoch 756/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 757/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 758/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 759/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0122 - val_accuracy: 0.9989\n",
            "Epoch 760/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 761/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0125 - val_accuracy: 0.9989\n",
            "Epoch 762/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 763/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0115 - val_accuracy: 0.9990\n",
            "Epoch 764/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0118 - val_accuracy: 0.9989\n",
            "Epoch 765/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 766/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 767/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0103 - val_accuracy: 0.9990\n",
            "Epoch 768/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 769/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0102 - val_accuracy: 0.9991\n",
            "Epoch 770/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 771/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 772/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 773/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 774/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0122 - val_accuracy: 0.9989\n",
            "Epoch 775/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 776/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
            "Epoch 777/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9989\n",
            "Epoch 778/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9989\n",
            "Epoch 779/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 780/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 781/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 782/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 783/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0121 - val_accuracy: 0.9990\n",
            "Epoch 784/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0122 - val_accuracy: 0.9989\n",
            "Epoch 785/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0115 - val_accuracy: 0.9990\n",
            "Epoch 786/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 787/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9989\n",
            "Epoch 788/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9990\n",
            "Epoch 789/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 790/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 791/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 792/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9990\n",
            "Epoch 793/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 794/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
            "Epoch 795/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9989\n",
            "Epoch 796/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9989\n",
            "Epoch 797/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9990\n",
            "Epoch 798/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 799/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 800/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 801/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0124 - val_accuracy: 0.9988\n",
            "Epoch 802/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0120 - val_accuracy: 0.9990\n",
            "Epoch 803/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0121 - val_accuracy: 0.9989\n",
            "Epoch 804/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 805/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0106 - val_accuracy: 0.9991\n",
            "Epoch 806/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0101 - val_accuracy: 0.9991\n",
            "Epoch 807/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 808/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 809/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0104 - val_accuracy: 0.9991\n",
            "Epoch 810/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0100 - val_accuracy: 0.9990\n",
            "Epoch 811/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 812/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9991\n",
            "Epoch 813/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 814/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 815/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0106 - val_accuracy: 0.9991\n",
            "Epoch 816/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 817/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0120 - val_accuracy: 0.9989\n",
            "Epoch 818/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9990\n",
            "Epoch 819/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9989\n",
            "Epoch 820/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9990\n",
            "Epoch 821/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 822/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 823/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 824/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 825/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 826/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0104 - val_accuracy: 0.9990\n",
            "Epoch 827/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
            "Epoch 828/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 829/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9990\n",
            "Epoch 830/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 831/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 832/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 833/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9990\n",
            "Epoch 834/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 835/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 836/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 837/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
            "Epoch 838/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 839/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
            "Epoch 840/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
            "Epoch 841/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 842/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 843/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 844/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 845/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
            "Epoch 846/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 847/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 848/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 849/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
            "Epoch 850/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9990\n",
            "Epoch 851/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 852/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 853/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 854/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9990\n",
            "Epoch 855/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
            "Epoch 856/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 857/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 858/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 859/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9991\n",
            "Epoch 860/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
            "Epoch 861/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0104 - val_accuracy: 0.9990\n",
            "Epoch 862/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0105 - val_accuracy: 0.9990\n",
            "Epoch 863/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 864/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9990\n",
            "Epoch 865/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0115 - val_accuracy: 0.9990\n",
            "Epoch 866/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
            "Epoch 867/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9990\n",
            "Epoch 868/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0126 - val_accuracy: 0.9990\n",
            "Epoch 869/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 870/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9990\n",
            "Epoch 871/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 872/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 873/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 874/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0108 - val_accuracy: 0.9990\n",
            "Epoch 875/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9991\n",
            "Epoch 876/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0103 - val_accuracy: 0.9991\n",
            "Epoch 877/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 878/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 879/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9990\n",
            "Epoch 880/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9991\n",
            "Epoch 881/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
            "Epoch 882/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 883/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 884/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 885/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0104 - val_accuracy: 0.9991\n",
            "Epoch 886/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0103 - val_accuracy: 0.9991\n",
            "Epoch 887/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0098 - val_accuracy: 0.9991\n",
            "Epoch 888/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0099 - val_accuracy: 0.9991\n",
            "Epoch 889/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0103 - val_accuracy: 0.9991\n",
            "Epoch 890/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 891/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9991\n",
            "Epoch 892/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0103 - val_accuracy: 0.9991\n",
            "Epoch 893/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0102 - val_accuracy: 0.9991\n",
            "Epoch 894/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 895/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 896/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 897/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 898/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 899/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 900/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0110 - val_accuracy: 0.9990\n",
            "Epoch 901/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 902/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 903/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 904/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 905/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 906/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 907/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
            "Epoch 908/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 909/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 910/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 911/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 912/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9991\n",
            "Epoch 913/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 914/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0104 - val_accuracy: 0.9991\n",
            "Epoch 915/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 916/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0106 - val_accuracy: 0.9991\n",
            "Epoch 917/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 918/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
            "Epoch 919/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 920/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 921/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 922/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 923/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 924/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 925/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0118 - val_accuracy: 0.9991\n",
            "Epoch 926/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9990\n",
            "Epoch 927/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9991\n",
            "Epoch 928/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 929/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 930/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 931/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9992\n",
            "Epoch 932/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0105 - val_accuracy: 0.9991\n",
            "Epoch 933/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 934/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 935/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 936/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 937/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 938/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 939/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
            "Epoch 940/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 941/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 942/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 943/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 944/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 945/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 946/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 947/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 948/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 949/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 950/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 951/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 952/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 953/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 954/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 955/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 956/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 957/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
            "Epoch 958/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 959/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0110 - val_accuracy: 0.9991\n",
            "Epoch 960/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 961/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 962/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 963/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 964/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9991\n",
            "Epoch 965/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 966/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
            "Epoch 967/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 968/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9991\n",
            "Epoch 969/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 970/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
            "Epoch 971/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 972/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0118 - val_accuracy: 0.9991\n",
            "Epoch 973/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0119 - val_accuracy: 0.9991\n",
            "Epoch 974/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0117 - val_accuracy: 0.9991\n",
            "Epoch 975/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0119 - val_accuracy: 0.9991\n",
            "Epoch 976/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
            "Epoch 977/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 978/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0111 - val_accuracy: 0.9991\n",
            "Epoch 979/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 980/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
            "Epoch 981/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 982/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9991\n",
            "Epoch 983/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0122 - val_accuracy: 0.9991\n",
            "Epoch 984/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9991\n",
            "Epoch 985/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0119 - val_accuracy: 0.9991\n",
            "Epoch 986/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9991\n",
            "Epoch 987/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0115 - val_accuracy: 0.9992\n",
            "Epoch 988/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0120 - val_accuracy: 0.9991\n",
            "Epoch 989/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0125 - val_accuracy: 0.9991\n",
            "Epoch 990/1000\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9991\n",
            "Epoch 991/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0123 - val_accuracy: 0.9991\n",
            "Epoch 992/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9991\n",
            "Epoch 993/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0116 - val_accuracy: 0.9991\n",
            "Epoch 994/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0112 - val_accuracy: 0.9992\n",
            "Epoch 995/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
            "Epoch 996/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0118 - val_accuracy: 0.9991\n",
            "Epoch 997/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9991\n",
            "Epoch 998/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 999/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0114 - val_accuracy: 0.9991\n",
            "Epoch 1000/1000\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0113 - val_accuracy: 0.9991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7QyynNWV2TTz",
        "outputId": "0f9cc159-f566-4ab4-fc8b-55274da6f852"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "NgORchcM22xO",
        "outputId": "35ddd933-508e-44e2-a34f-406043d5d296"
      },
      "source": [
        "plt.clf()\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqM1Yeif-mms"
      },
      "source": [
        "Now, our model starts overfitting after 400 epochs. The validation accuracy has also improved somewhat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry0E0PJz-0sb"
      },
      "source": [
        "# Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP5gEN5w-yo8"
      },
      "source": [
        "model3 = models.Sequential()\n",
        "model3.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
        "model3.add(layers.Dense(2, activation='relu'))\n",
        "model3.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model3.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model3.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=512,\n",
        "    epochs=400,\n",
        "    verbose=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4os77jk-cOY",
        "outputId": "5a121cae-3eaa-414a-df04-f6a9dddd62a8"
      },
      "source": [
        "result = model3.evaluate(x_test, y_test)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2660/2660 [==============================] - 3s 1ms/step - loss: 0.0052 - accuracy: 0.9994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.005246598739176989, 0.99943608045578]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5FGW9cn_zej"
      },
      "source": [
        "# Final Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvQeHNKz_uPc",
        "outputId": "b5eee010-f5ee-403a-b8d7-6630b928abfb"
      },
      "source": [
        "print(\"Model Accuracy for Test Data: {acc:.2f}%\".format(acc=result[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy for Test Data: 99.94%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}